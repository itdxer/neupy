<!DOCTYPE html><!--[if lt IE 7]>      <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="description" content="NeuPy is a Python library for Artificial Neural Networks. NeuPy supports many different types of Neural Networks from a simple perceptron to deep learning models.">
        <meta name="viewport" content="width=device-width">
        <title>Articles &mdash; NeuPy</title>
            <link rel="stylesheet" href="_static/normalize.css" type="text/css">
            <link rel="stylesheet" href="_static/sphinx.css" type="text/css">
            <link rel="stylesheet" href="_static/main.css" type="text/css">
            <link rel="stylesheet" href="_static/flat.css" type="text/css">
            <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
            <link rel="stylesheet" href="_static/font-awesome.min.css" type="text/css">
        <link rel="shortcut icon" href="_static/favicon.ico" /><!-- Load modernizr and JQuery -->
        <script src="_static/vendor/modernizr-2.6.2.min.js"></script>
        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"></script>
        <script>window.jQuery || document.write('<script src="_static/vendor/jquery-1.8.2.min.js"><\/script>')</script>
        <script src="_static/plugins.js"></script>
        <script src="_static/main.js"></script>
        <link rel="alternate" type="application/rss+xml" title="RSS" href="rss.html" /><script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.5',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script><script type="text/javascript" src="_static/underscore.js"></script><script type="text/javascript" src="_static/doctools.js"></script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="_static/disqus.js"></script><script type="text/javascript" src="_static/js/google_analytics.js"></script><script type="text/javascript" src="_static/js/script.js"></script><script type="text/javascript" src="_static/js/copybutton.js"></script>

    <script type="text/javascript">
        $(document).ready(function () {
            // Scroll to content if on small screen
            if (screen.width < 480)
            {
                $(document).scrollTop(document.getElementsByTagName("article")[0].offsetTop - 44);
            }
        });
    </script>
    <style media="screen" type="text/css">
        .docutils { width: 100%; }
        .docutils td { padding: 10px; }
        .section { word-wrap:break-word; }
        .descname { font-weight: bold; }
        .highlight-python + .figure { margin-top: 20px; }
        .dataframe { text-align: center !important; width: 100%; margin: 10px 0 10px 0; }
        .dataframe td { padding: 5px; }

        .math .gd { color: #000 !important; } /* Generic.Deleted */
        .math .m { color: #000 !important; } /* Literal.Number */
        .math .s { color: #000 !important; } /* Literal.String */
        .math .mf { color: #000 !important; } /* Literal.Number.Float */
        .math .mh { color: #000 !important; } /* Literal.Number.Hex */
        .math .mi { color: #000 !important; } /* Literal.Number.Integer */
        .math .mo { color: #000 !important; } /* Literal.Number.Oct */
        .math .sc { color: #000 !important; } /* Literal.String.Char */
        .math .s2 { color: #000 !important; } /* Literal.String.Double */
        .math .si { color: #000 !important; } /* Literal.String.Interpol */
        .math .sx { color: #000 !important; } /* Literal.String.Other */
        .math .s1 { color: #000 !important; } /* Literal.String.Single */
        .math .ss { color: #000 !important; } /* Literal.String.Symbol */
        .math .il { color: #000 !important; } /* Literal.Number.Integer.Long */

        /* Background for class and function names */
        dt[id^="neupy."] {
            background-color: #e6edf2;
            border: 1px solid #f8fafb;
            border-radius: 8px;
            padding: 10px 20px;
        }
        div[id^="module-neupy."] h1 {
            display: none;
        }

        /* Search input field */
        .search-input {
            width: 100%;
            padding: 10px;
            display: block;
        }
        .box {
          padding-bottom: 50px;
        }
        .search-input-container {
          width: 100%;
          vertical-align: middle;
          white-space: nowrap;
          position: relative;
        }
        .search-input-container input#search {
          width: 100%;
          height: 50px;
          padding-left: 45px;

          float: left;
          outline: none;
          border: 1px solid #ddd;

          box-sizing: border-box;
          -webkit-box-sizing: border-box;
          -moz-box-sizing: border-box;

          -webkit-border-radius: 5px;
          -moz-border-radius: 5px;
          border-radius: 5px;

          font-family: 'PT Sans', Helvetica, Arial, sans-serif;
          font-size: 12pt;
        }
        .search-input-container .icon {
          position: absolute;
          left: 0;
          top: 50%;
          margin-left: 17px;
          margin-top: 13px;
          z-index: 1;
          color: #93a4ad;
        }

        #search-results ul.search {
            padding: 0;
        }
        #search-results li p {
            margin-bottom: 5px;
            font-size: 0.9em;
        }
        #search-results li {
            background-color: #fff !important;
            margin-bottom: 10px;
            display: block !important;
            padding: 15px;
            border-bottom: 2px solid #ddd;
        }
        #search-results span.tag {
            display: inline-block;
            padding: 3px 4px;
            margin-right: 8px;
            position: relative;
            top: -2px;

            background: #888;
            color: #fff;

            font-size: 0.75em;
            font-weight: 600;
            line-height: 1;
            text-transform: uppercase;

            -webkit-border-radius: 2px;
            -moz-border-radius: 2px;
            border-radius: 2px;
        }
    </style></head>
    <body role="document">
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a> to improve your experience.</p>
        <![endif]-->

      <div id="container"><header role="banner">
        <div>
          <h1><a href="pages/home.html">NeuPy</a></h1>
          <h2>Neural Networks in Python</h2>
        </div>
    </header>
    <nav role="navigation">
      <ul>
        <li class="main-nav">
          <a href="archive.html">Articles</a>
        </li>
        <li class="main-nav">
          <a href="docs/tutorials.html">Tutorials</a>
        </li>
        <li class="main-nav">
          <a href="pages/documentation.html">Documentation</a>
        </li>
        <li class="main-nav">
          <a href="pages/cheatsheet.html">Cheat sheet</a>
        </li>
        <li class="main-nav">
          <a href="pages/model_zoo.html">Model Zoo</a>
        </li>
      </ul>
    </nav>

<div class="main-container" role="main"><div class="main wrapper body clearfix"><article><div class="timestamp postmeta">
            <span>March 26, 2018</span>
        </div>
        <div class="section">
            <h1><a href="2018/03/26/making_art_with_growing_neural_gas.html">Making Art with Growing Neural Gas</a></h1>
<div class="figure align-center">
<img alt="Art generated using Growing Neural Gas in NeuPy" src="_images/gng-art-final.png"/>
</div>
<div class="section" id="introduction">
<h2>Introduction</h2>
<p>I’ve been trying to make that type of art style for quite some time. I applied <a class="reference external" href="http://neupy.com/apidocs/neupy.algorithms.competitive.sofm.html#neupy.algorithms.competitive.sofm.SOFM">SOFM</a> to the images, but in most cases it was unsuccessful, mostly because SOFM requires predefined size and structure of the network. With such a requirement it’s difficult to construct tool that converts image to nice art style. Later, I’ve learned more about <a class="reference external" href="http://neupy.com/apidocs/neupy.algorithms.competitive.growing_neural_gas.html#neupy.algorithms.competitive.growing_neural_gas.GrowingNeuralGas">Growing Neural Gas</a> and it helped to resolve main issues with SOFM. In this article, I want to explain how this type of art style can be generated from the image. At the end, I will cover some of the similar, but less successful application with growing neural gas for image processing that I’ve been trying to develop.</p>
</div>
<div class="section" id="image-processing-pipeline">
<h2>Image Processing Pipeline</h2>
<p>Images are not very natural data structure for most of the machine learning algorithms and Growing Neural Gas (GNG) is not an exception. For this reason, we need to represent input image in format that will be understandable for the network. The right format for the GNG would be set of data points. In addition, these data points have to somehow resemble original image. In order to do it, we can binarize our image and after that, every pixel on the image will be either black or white. Each black pixel we can use as a data point and pixel’s position as a feature. In this way, we would be able to extract topological structure of the image and store it as set of data points.</p>
<p>Conversion from the color image to binary image requires three simple image processing steps that we will apply in sequential way.</p>
<ol class="arabic">
<li><p class="first">We need to load our image first</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="c1"># skimage version 0.13.1</span>
<span class="kn">from</span> <span class="nn">skimage</span> <span class="kn">import</span> <span class="n">data</span><span class="p">,</span> <span class="n">img_as_float</span>

<span class="n">astro</span> <span class="o">=</span> <span class="n">img_as_float</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">astronaut</span><span class="p">())</span>
<span class="n">astro</span> <span class="o">=</span> <span class="n">astro</span><span class="p">[</span><span class="mi">30</span><span class="p">:</span><span class="mi">180</span><span class="p">,</span> <span class="mi">150</span><span class="p">:</span><span class="mi">300</span><span class="p">]</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="Astronaut image" src="_images/colored-image.png"/>
</div>
</li>
<li><p class="first">Convert color image to grayscale</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">skimage</span> <span class="kn">import</span> <span class="n">color</span>
<span class="n">astro_grey</span> <span class="o">=</span> <span class="n">color</span><span class="o">.</span><span class="n">rgb2grey</span><span class="p">(</span><span class="n">astro</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="Grayscale image of an astronaut" src="_images/grey-image.png"/>
</div>
</li>
<li><p class="first">Apply gaussian blurring. It will allow us to reduce image detalization.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">skimage.filters</span> <span class="kn">import</span> <span class="n">gaussian</span>
<span class="n">blured_astro_grey</span> <span class="o">=</span> <span class="n">gaussian</span><span class="p">(</span><span class="n">astro_grey</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="Blurred and grey scaled astronaut image" src="_images/blured-image.png"/>
</div>
</li>
<li><p class="first">Find binarization threshold and convert to the black color every pixel that below this threshold.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">skimage.filters</span> <span class="kn">import</span> <span class="n">threshold_otsu</span>
<span class="c1"># Increase threshold in order to add more</span>
<span class="c1"># details to the binarized image</span>
<span class="n">thresh</span> <span class="o">=</span> <span class="n">threshold_otsu</span><span class="p">(</span><span class="n">astro_grey</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span>
<span class="n">binary_astro</span> <span class="o">=</span> <span class="n">astro_grey</span> <span class="o">&lt;</span> <span class="n">thresh</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="Binarized astronaut image" src="_images/binary-image.png"/>
</div>
<p>In some cases, it might be important to adjust threshold in order to be able to capture all important details. In this example, I added <cite>0.1</cite> to the threshold.</p>
</li>
</ol>
<p>And finally, from the binary image it’s easy to make data points.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">ndenumerate</span><span class="p">(</span><span class="n">binary_astro</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">value</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">y</span><span class="p">,</span> <span class="o">-</span><span class="n">x</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="Image represented as a set of data points" src="_images/data-points-scatter-plot.png"/>
</div>
<p>In the image there are so many data points that it’s not clear if it’s really just a set of data points. But if you zoom in you will see that they really are.</p>
<div class="figure align-center">
<img alt="../../../_images/data-points-eye-scatter-plot.png" src="_images/data-points-eye-scatter-plot.png"/>
</div>
<p>We prepared our data and now we need to learn a bit more about GNG network.</p>
</div>
<div class="section" id="id1">
<h2>Growing Neural Gas</h2>
<div class="figure align-center">
<img alt="Growing Neural Gas animation in NeuPy" src="_images/neural-gas-animation.gif"/>
</div>
<p>Growing Neural Gas is very simple algorithm and it’s really easy to visualize it. From the animation above you can see how it learns shape of the data. Network, typically, starts with two random points and expands over the space.</p>
<p>In the original paper <a class="footnote-reference" href="#id4" id="id2">[1]</a>, algorithm looks a bit complicated with all variables and terminology, but in reality it’s quite simple. Simplified version of the algorithm might look like this:</p>
<ol class="arabic">
<li><p class="first">Pick one data point at random (red data point).</p>
<div class="figure align-center">
<img alt="Growing Neural Gas - data sampling" src="_images/gng-sampled-point-with-graph.png"/>
</div>
<p>Blue region represents large set of data points that occupy space in the form of a unit circle. And green points connected with black lines is our GNG network. Green points are neurons and black line visualize connection between two neurons.</p>
</li>
<li><p class="first">Find two closest neurons (blue data points) to the sampled data point and connect these neurons with an edge.</p>
<div class="figure align-center">
<img alt="Growing Neural Gas - adding new edge" src="_images/gng-added-edge.png"/>
</div>
</li>
<li><p class="first">Move closest neuron towards the data point. In addition, you can move neurons, that connected by the edge with closest neuron, towards the same point.</p>
<div class="figure align-center">
<img alt="Growing Neural Gas - update neuron weights (coordinates)" src="_images/gng-updated.png"/>
</div>
</li>
<li><p class="first">Each neuron has error that accumulates over time. For every updated neuron we have to increase error. Increase per each neuron equal to the distance (euclidean) from this neuron to the sampled data point. The further the neuron from the data point the larger the error.</p>
</li>
<li><p class="first">Remove edges that haven’t been updated for a while (maybe after 50, 100 or 200 iterations, up to you). In case if there are any neurons that doesn’t have edges then we can remove them too.</p>
</li>
</ol>
<div class="figure align-center">
<img alt="Growing Neural Gas - remove old edges" src="_images/gng-edge-removed.png"/>
</div>
<ol class="arabic" start="6">
<li><p class="first">From time to time (maybe every 100 or 200 iterations) we can find neuron that has largest accumulated error. For this neuron we can find it’s neighbour with the highest accumulated error. In the middle way between them we can create new neuron (blue data point) that will be automatically connected to these two neurons and original edge between them will be destroyed.</p>
<div class="figure align-center">
<img alt="Growing Neural Gas - adding new neuron" src="_images/gng-new-neuron-added.png"/>
</div>
<p>You can think about this step in the following way. Find neuron that typically makes most errors and add one more neuron near it. This new neuron will help the other neuron to reduce accumulated error. Reduction in error will mean that we better capture structure of our data.</p>
</li>
<li><p class="first">Repeat all the steps many times.</p>
</li>
</ol>
<p>There are a few small extensions to the algorithm has to be added in order to be able to call it Growing Neural Gas, but the most important principles are there.</p>
</div>
<div class="section" id="putting-everything-together">
<h2>Putting Everything Together</h2>
<p>And now we ready to combine power of the image processing pipeline with Growing Neural Gas.</p>
<p>After running for one epoch we can already see some progress. Generated network resembles some distinctive features of our original image. At this point it’s pretty obvious that we don’t have enough neurons in the network in order to capture more details.</p>
<div class="figure align-center">
<img alt="Growing Neural Gas art generation in Neupy - 1st epoch" src="_images/gng-art-epoch-1.png"/>
</div>
<p>After 4 more iterations, image looks much closer to the original. You can notice that regions with large amount of data points have been developed properly, but small features like eyes, nose and mouth hasn’t been formed yet. We just have to wait more.</p>
<div class="figure align-center">
<img alt="Growing Neural Gas art generation in Neupy - 5th epoch" src="_images/gng-art-epoch-5.png"/>
</div>
<p>After 5 more iterations the eyebrows and eyes have better quality. Even hair has more complex shape.</p>
<div class="figure align-center">
<img alt="Growing Neural Gas art generation in Neupy - 10th epoch" src="_images/gng-art-epoch-10.png"/>
</div>
<p>On the 20th iteration network’s training has been stopped since we achieved desired quality of the image.</p>
<div class="figure align-center">
<img alt="Growing Neural Gas art generation in Neupy - 20th epoch" src="_images/gng-art-epoch-20.png"/>
</div>
</div>
<div class="section" id="reveal-issues-with-more-examples">
<h2>Reveal Issues with More Examples</h2>
<p>I’ve been doing some experiments with other image as well, and there are a few problems that I’ve encountered.</p>
<p>There are two main components in the art style generation procedure, namely: image processing pipeline and GNG. Let’s look at problem with GNG network. It can be illustrated with the following image.</p>
<div class="figure align-center">
<img alt="Horse image generated using Growing Neural Gas in NeuPy" src="_images/horses.png"/>
</div>
<p>If you compare horses you will notice that horse on the right image looks a bit skinnier than the left one. It happened, because neurons in the GNG network are not able to rich edges of the image. After one training pass over the full dataset each neuron is getting pulled from many directions and over the training process it sattels somewhere in the middle, in order to be as close as possible to every sample that pulls it. The more neurons you add to the network the closer it will get to the edge.</p>
<p>Another problem related to the image binarization, the most difficult step in our image processing pipeline. It’s difficult, because each binarization method holds certain set of assumption that can easily fail for different images and there is no general way to do it. You don’t have such a difficulty with the network. It can give you pretty decent results for different images using the same configurations. The only thing that you typically need to control is the maximum number of neurons in the network. The more neuron you allow network to use the better quality of the image it produces.</p>
<p>In this article, I used global binarization method for image processing. This type of binarization generates single threshold for all pixels in the image, which can cause problems. Let’s look at the image below.</p>
<div class="figure align-center">
<img alt="Man with camera in the image generated using Growing Neural Gas in NeuPy" src="_images/camera-man.png"/>
</div>
<p>You can see that that there are some building in the background in the left image, but there is none in the right one. It’s hard to capture multiple object using single threshold, especially when they have different shades. For more complex cases you might try to use local thresholding methods.</p>
</div>
<div class="section" id="applying-similar-approach-to-text">
<h2>Applying Similar Approach to Text</h2>
<p>I’ve been also experimenting with text images. In the image below you can see the result.</p>
<div class="figure align-center">
<img alt="Writing text using Growing Neural Gas" src="_images/text-in-page.png"/>
</div>
<p>It’s even possible to read text generated by the network. It’s also interesting that with slight modification to the algorithm you can count number of words in the image. We just need to add more blurring and after the training - count number of subgraphs in the network.</p>
<div class="figure align-center">
<img alt="Blured and binarized text image" src="_images/blured-text-binarized.png"/>
</div>
<p>After many reruns I typically get number that very close to the right answer (44 words if you count “Region-based” as two words).</p>
<p>I also tried to train GNG network that captures trajectory of the signature. There are a few issues that I couldn’t overcome. In the image below you can clearly see some of these issues.</p>
<div class="figure align-center">
<img alt="Writing signatures using Growing Neural Gas in NeuPy" src="_images/signature.png"/>
</div>
<p>You will expect to see a signature as a continuous line and this property is hard to achieve using GNG. In the image above you can see a few places where network tries to cover some regions with small polygons and lines which looks very unnatural.</p>
</div>
<div class="section" id="final-words">
<h2>Final Words</h2>
<p>Beautiful patterns generated from the images, probably, doesn’t reflect the real power of GNG network, but I think that the beauty behind algorithm shouldn’t be underappreciated only because it’s not useful for solving real world problems. There are not many machine learning algorithms that can be used for artistic application and it’s pretty cool when they work even though they weren’t designed for this purpose.</p>
<p>I had a lot of fun trying different ideas and I encourage you to try it as well. If you’re new to machine learning - it’s easy to start with GNG and if you’re an expert, I might try motivating you saying that it’s quite refreshing to work with neural networks that can be easily interpreted and analyzed.</p>
</div>
<div class="section" id="learn-more">
<h2>Learn More</h2>
<p>In case if you want to learn more about algorithms just like GNG then you can read about <a class="reference external" href="http://neupy.com/2017/12/09/sofm_applications.html">SOFM</a>. As I said in the beginning of the article, it doesn’t work as nice as GNG for images, but you can write <a class="reference external" href="http://neupy.com/2017/12/17/sofm_text_style.html">pretty cool text styles</a> or generate <a class="reference external" href="http://neupy.com/2017/12/13/sofm_art.html">beautiful patterns</a>. And, it has some other <a class="reference external" href="http://neupy.com/2017/12/09/sofm_applications.html#applications">interesting applications</a> (even in <a class="reference external" href="http://neupy.com/2017/12/09/sofm_applications.html#visualize-pre-trained-vgg19-network">deep learning</a>).</p>
</div>
<div class="section" id="code">
<h2>Code</h2>
<p>A few notebooks with code are available on github.</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/itdxer/neupy/blob/master/notebooks/growing-neural-gas/Making%20Art%20with%20Growing%20Neural%20Gas.ipynb">Main notebook</a> that generates all the images using GNG</li>
<li><a class="reference external" href="https://github.com/itdxer/neupy/blob/master/notebooks/growing-neural-gas/Growing%20Neural%20Gas%20animated.ipynb">Growing Neural Gas animation notebook</a></li>
<li>Notebook that generates <a class="reference external" href="https://github.com/itdxer/neupy/blob/master/notebooks/growing-neural-gas/Growing%20Neural%20Gas%20-%20step%20by%20step%20visualizations.ipynb">step by step visualization images for the Growing Neural Gas</a> algorithm</li>
</ul>
</div>
<div class="section" id="references">
<h2>References</h2>
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[1]</a></td><td>A Growing Neural Gas Network Learns Topologies, Bernd Fritzke et al. <a class="reference external" href="https://papers.nips.cc/paper/893-a-growing-neural-gas-network-learns-topologies.pdf">https://papers.nips.cc/paper/893-a-growing-neural-gas-network-learns-topologies.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Thresholding, tutorial from scikit-image library <a class="reference external" href="http://scikit-image.org/docs/dev/auto_examples/xx_applications/plot_thresholding.html">http://scikit-image.org/docs/dev/auto_examples/xx_applications/plot_thresholding.html</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id6" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>Thresholding (image processing), wikipedia article <a class="reference external" href="https://en.wikipedia.org/wiki/Thresholding_%28image_processing%29">https://en.wikipedia.org/wiki/Thresholding_%28image_processing%29</a></td></tr>
</tbody>
</table>
</div>

        </div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Yurii Shevchuk</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="tags/image_processing.html">image processing</a>, <a href="tags/unsupervised.html">unsupervised</a>, <a href="tags/art.html">art</a></span>
        </div>
        <div class="comments">
            <a href="http://neupy.com/2018/03/26/making_art_with_growing_neural_gas.html#disqus_thread" data-disqus-identifier="2018/03/26/making_art_with_growing_neural_gas">Leave a comment</a>
        </div></div><div class="separator post_separator"></div><div class="timestamp postmeta">
            <span>December 17, 2017</span>
        </div>
        <div class="section">
            <h1><a href="2017/12/17/sofm_text_style.html">Create unique text-style with SOFM</a></h1>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/neupy-logo.png"><img alt="NeuPy logo" src="_images/neupy-logo.png" style="width: 100%;"/></a>
</div>
<div class="section" id="introduction">
<h2>Introduction</h2>
<p>In this article, I want to show how to generate unique text style using Self-Organizing Feature Maps (SOFM). I won’t explain how SOFM works in this article, but if you want to learn more about algorithm you can check these articles.</p>
<ol class="arabic simple">
<li><a class="reference internal" href="2017/12/09/sofm_applications.html#sofm-applications"><span>Self-Organizing Maps and Applications</span></a></li>
<li><a class="reference internal" href="2017/12/13/sofm_art.html#sofm-art"><span>The Art of SOFM</span></a></li>
</ol>
</div>
<div class="section" id="transforming-text-into-the-data">
<h2>Transforming text into the data</h2>
<p>In order to start, we need to have some text prepared. I generated image using matplotlib, but anything else that able to generate image from the text will work.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">red</span><span class="p">,</span> <span class="n">blue</span><span class="p">,</span> <span class="n">white</span> <span class="o">=</span> <span class="p">(</span><span class="s1">'#E24A33'</span><span class="p">,</span> <span class="s1">'#348ABD'</span><span class="p">,</span> <span class="s1">'#FFFFFF'</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">patch</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="n">white</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="s1">'NeuPy'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">'neupy-text.png'</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="n">white</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">'tight'</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/neupy-text.png"><img alt="NeuPy raw text" src="_images/neupy-text.png" style="width: 50%;"/></a>
</div>
<p>We cannot train SOFM using image, for this reason we will have to transform image into a set of data points. In order to do it we will encode every black pixel as data point and we will ignore white pixels. It’s hard to see from the picture, but not all pixels are black and white. If you zoom close enough you will see that there are some gray pixels near the edge of each letter. For this reason, we have to binarize our image first.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">scipy.misc</span> <span class="kn">import</span> <span class="n">imread</span>

<span class="n">neupy_text</span> <span class="o">=</span> <span class="n">imread</span><span class="p">(</span><span class="s1">'neupy-text.png'</span><span class="p">)</span>
<span class="c1"># Encode black pixels as 1 and white pixels as 0</span>
<span class="n">neupy_text</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">neupy_text</span> <span class="o">/</span> <span class="mf">255.</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>After binarization we have to filter all pixels that have value 1 and use pixel coordinates as a data point coordinates.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">ndenumerate</span><span class="p">(</span><span class="n">neupy_text</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">value</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">y</span><span class="p">,</span> <span class="o">-</span><span class="n">x</span> <span class="o">+</span> <span class="mi">300</span><span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>We can use scatter plot to show that collected data points still resemble the shape of the main text.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/neupy-text-data-points.png"><img alt="NeuPy text represented as set of data points" src="_images/neupy-text-data-points.png" style="width: 80%;"/></a>
</div>
</div>
<div class="section" id="weight-initialization">
<h2>Weight initialization</h2>
<p>Weight initialization is very important step. With default random initialization it can be difficult for the network to cover the text, since in many cases neurons will have to travel across all image in order to get closer to their neighbors. In order to avoid this issue we have to manually generate grid of neurons, so that it would be easier for the network to cover the text.</p>
<p>I tried many different patterns and most of them work, but one-dimensional or <em>nearly</em> one-dimensional grids produced best patterns. It’s mostly because patterns generated using two-dimensional grid look very similar to each other. With one-dimensional grid it’s like covering the same text with long string. Network will be forced to stretch and rollup in order to cover the text. I mentioned term <em>nearly</em> one-dimensional, because that’s the shape that I used at the end. Term “nearly” means that grid is two-dimensional, but because number of neurons along one dimension much larger than along the other we can think of it as almost one-dimensional. In the final solution I used grid with shape 2x1000.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="c1"># This parameter will be also used in the SOFM</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Generate weights and arange them along sine wave.</span>
<span class="c1"># Because sine way goes up and down the final pattern</span>
<span class="c1"># will look more interesting.</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>

<span class="c1"># Width of the weights were selected specifically for the NeuPy text</span>
<span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

<span class="c1"># Amplitute of the sine function also was selected in order</span>
<span class="c1"># to roughly match height of the text</span>
<span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">50</span> <span class="o">+</span> <span class="mi">50</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">weight</span><span class="p">,</span> <span class="n">weight</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>You can notice from the code that I applied sine function on the y-axis coordinates of the grid. With two-dimensional grid it’s easy to cover the text. We just put large rectangular grid over the text. With nearly one-dimensional grid it’s a bit tricky. We need to have a way that will allow us to cover our text and sine is one of the simple functions that can provide such a property. From the image below you can see how nicely it cover our text.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">weight</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">red</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/initialized-sofm-weights.png"><img alt="Initialized SOFM weights along sine wave" src="_images/initialized-sofm-weights.png" style="width: 100%;"/></a>
</div>
</div>
<div class="section" id="training-network">
<h2>Training network</h2>
<p>And the last step is to train the network. It took me some time to find right parameters for the network. Typically it was easy to see that there is something wrong with a training when all neurons start forming strange shapes that look nothing like the text. The main problem I found a bit latter. Because we have roughly 20,000 data points and 2000 neurons we make to many updates during one iteration without reducing parameter values. Reducing step size helped to solve this issue, because every update makes small change to the grid and making lots of these small changes make noticeable difference.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span>

<span class="n">sofm</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">SOFM</span><span class="p">(</span>
    <span class="n">n_inputs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">features_grid</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>

    <span class="c1"># With large number of training samples it's safer</span>
    <span class="c1"># to use small step (learning rate)</span>
    <span class="n">step</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>

    <span class="c1"># Learning radis large for first 10 iterations, after that we</span>
    <span class="c1"># assume that neurons found good positions on the text and we just</span>
    <span class="c1"># need to move them a bit independentl in order to cover text better</span>
    <span class="n">learning_radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>

    <span class="c1"># after 10 iteration learning radius would be 0</span>
    <span class="n">reduce_radius_after</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>

    <span class="c1"># slowly decrease step size</span>
    <span class="n">reduce_step_after</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Because of the small step size we have to do more training iterations. It takes more time to converge, but final results are more stable to some changes in the input data. It’s possible to speed up the overall process tuning parameter more carefully, but I decided that it’s good enough.</p>
<p>I run training procedure for 30 iterations.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">sofm</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Training iteration #{}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">iteration</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">sofm</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-training-iteration-1.png"><img alt="SOFM training iteration #1" src="_images/sofm-training-iteration-1.png" style="width: 80%;"/></a>
</div>
<p>SOFM was trained for only one iteration and we already can vaguely see most of the latters. Let’s wait a few more iterations.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-training-iteration-3.png"><img alt="SOFM training iteration #3" src="_images/sofm-training-iteration-3.png" style="width: 80%;"/></a>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-training-iteration-5.png"><img alt="SOFM training iteration #5" src="_images/sofm-training-iteration-5.png" style="width: 80%;"/></a>
</div>
<p>Now it’s way more clear that network makes progress during the training. And after 5 more iterations it’s almost perfectly covers text.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-training-iteration-10.png"><img alt="SOFM training iteration #10" src="_images/sofm-training-iteration-10.png" style="width: 80%;"/></a>
</div>
<p>But even after 10 iterations we still can see that some of the letters still require some polishing. For instance, left part of the letter N hasn’t been properly covered.</p>
<p>In addition, it’s important to point out that we specified step reduction after every 10 iterations. It means that now we won’t move neurons as much as we did before. Also, learning radius was reduced to zero, which means that after 10th iteration each neuron will move independently. And these two changes are exactly what we need. We can see from the picture that network covers text pretty good, but small changes will make it look even better.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-training-iteration-15.png"><img alt="SOFM training iteration #15" src="_images/sofm-training-iteration-15.png" style="width: 80%;"/></a>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-training-iteration-30.png"><img alt="SOFM training iteration #30" src="_images/sofm-training-iteration-30.png" style="width: 80%;"/></a>
</div>
<p>You can notice that there is almost no difference between iteration #15 and #30. It doesn’t look like we made any progress after 15th iteration, but it’s not true. If you stop training after 15th iteration, you will notice that some parts of the letters look a bit odd. These 15 last iterations do small changes that won’t be noticeable from the scatter plot, but they are important.</p>
<p>And finally after all training iterations we can use our weights to generate logo.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="c1"># Function comes from the neupy's examples folder</span>
<span class="kn">from</span> <span class="nn">examples.competitive.utils</span> <span class="kn">import</span> <span class="n">plot_2d_grid</span>

<span class="n">background_color</span> <span class="o">=</span> <span class="s1">'#22264b'</span>
<span class="n">text_color</span> <span class="o">=</span> <span class="s1">'#e8edf3'</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">patch</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="n">background_color</span><span class="p">)</span>

<span class="n">sofm_weights</span> <span class="o">=</span> <span class="n">sofm</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">plot_2d_grid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">sofm_weights</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">color</span><span class="o">=</span><span class="n">text_color</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>

<span class="c1"># Coordinates were picked so that text</span>
<span class="c1"># will be in the center of the image</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">220</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">560</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/neupy-logo.png"><img alt="NeuPy logo" src="_images/neupy-logo.png" style="width: 100%;"/></a>
</div>
</div>
<div class="section" id="generalized-approach-for-any-text">
<h2>Generalized approach for any text</h2>
<p>There are some challenges that you can face when you try to adopt this solution for different text. First of all, from the code you could have noticed that I “hard-coded” bounds of the text. In more general solution they can be identified from the image, but it will make solution more complex. For instance, the right bound of the text can be associated with data point that has largest x-coordinate. And the same can be done for the upper bound of the text. Second problem is related to the parameters of the SOFM. The main idea was to make lots of small updates for a long time, but it might fail for some other text that has more letters, because we will have more data points and more updates during each iterations. Problem can be solved if step size will be reduced.</p>
</div>
<div class="section" id="further-reading">
<h2>Further reading</h2>
<p>If you want to learn more about SOFM, you can read the <a class="reference internal" href="2017/12/09/sofm_applications.html#sofm-applications"><span>“Self-Organizing Maps and Applications”</span></a> article that covers basic ideas behind SOFM and some of the problems that can be solved with this algorithm.</p>
</div>
<div class="section" id="code">
<h2>Code</h2>
<p>All the code that was used to generate images in the article you can find in <a class="reference external" href="https://github.com/itdxer/neupy/blob/master/notebooks/sofm/Generating%20NeuPy%20logo%20with%20SOFM.ipynb">iPython notebook on github</a>.</p>
</div>

        </div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Yurii Shevchuk</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="tags/sofm.html">sofm</a>, <a href="tags/unsupervised.html">unsupervised</a>, <a href="tags/art.html">art</a></span>
        </div>
        <div class="comments">
            <a href="http://neupy.com/2017/12/17/sofm_text_style.html#disqus_thread" data-disqus-identifier="2017/12/17/sofm_text_style">Leave a comment</a>
        </div></div><div class="separator post_separator"></div><div class="timestamp postmeta">
            <span>December 13, 2017</span>
        </div>
        <div class="section">
            <span id="sofm-art"/><h1><a href="2017/12/13/sofm_art.html">The Art of SOFM</a></h1>
<div class="section" id="introduction">
<h2>Introduction</h2>
<p>In this article, I just want to show how beautiful sometimes can be a neural network. I think, it’s quite rare that algorithm can not only extract knowledge from the data, but also produce something beautiful using exactly the same set of training rules without any modifications.</p>
</div>
<div class="section" id="the-main-idea">
<h2>The main idea</h2>
<p>SOFM consists of multiple neurons that typically arranged in the <strong>grid</strong> with connections between some of the neurons. In this article, we will use grid in the shape of the square that looks like this.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-grid1.png"><img alt="SOFM grid" src="_images/sofm-grid1.png" style="width: 30%;"/></a>
</div>
<p>During the training iteration we introduce some data point near the grid.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-before-update-with-one-data-point.png"><img alt="SOFM with one data point" src="_images/sofm-before-update-with-one-data-point.png" style="width: 70%;"/></a>
</div>
<p>SOFM finds neuron that closest to the introduced data point. After that, it pushes neuron towards this data point. In addition, it finds a few neighbour neurons within specified radius, that we call <strong>learning radius</strong>, and pushes these neurons towards data point as well, but not as much as it pushed closest neuron.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-update-with-one-data-point.png"><img alt="SOFM updated after one data point was introduced" src="_images/sofm-update-with-one-data-point.png" style="width: 70%;"/></a>
</div>
<p>To make it more interesting, we can put few extra points around the same grid.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-without-update.png"><img alt="SOFM state before the update" src="_images/sofm-without-update.png" style="width: 70%;"/></a>
</div>
<p>And applying one update per each data point at a time we can get nice pattern.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-4-dots-update.png"><img alt="SOFM symmetrically update weights" src="_images/sofm-4-dots-update.png" style="width: 70%;"/></a>
</div>
<p>We can make patterns look more interestingly if we start moving data points around. Let’s now use only two points and put one on the left and one on the right side from the grid. We do the same SOFM update again, but as soon as it’s done we rotate two data points by 45 degrees counterclockwise. After repeating this process a few times, we can get another pattern.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-update-and-rotate.png"><img alt="SOFM symmetrically update weights rotating data points" src="_images/sofm-update-and-rotate.png" style="width: 70%;"/></a>
</div>
<p>Black dots are two initial data points. And gray dots show places where we’ve seen these two data points after each new 45 degree rotation.</p>
<p>It’s pretty cool that this simple approach can produce such a beautiful patterns. We can even define set of variables changing which we can generate different patterns.</p>
</div>
<div class="section" id="making-patterns-more-interesting">
<h2>Making patterns more interesting</h2>
<p>Randomizing some of the SOFM parameters we can produce different patterns on each run. In this article, we will use 3 most important SOFM parameters:</p>
<ol class="arabic simple">
<li>Learning rate</li>
<li>Learning radius</li>
<li>Learning rate for the neighbour neurons</li>
</ol>
<p><strong>Learning rate</strong> defines by how much we will push neurons during the updated. If learning rate value is small then we won’t push it to far from the initial position.</p>
<p><strong>Learning radius</strong> defines how many neighbour neurons will be updated. The larger the radius the more neighbours would be updated after each iteration. If learning radius equal to zero then only one neuron will be updated.</p>
<p>And the last one is a parameter that controls <strong>learning rate for the neighbour neurons</strong>. The larger the value the bigger update neighbour neurons will get.</p>
</div>
<div class="section" id="generate-interesting-patterns">
<h2>Generate interesting patterns</h2>
<p>As in the example before, we will use only two data points and we will rotate them after each update by 45 degree counterclockwise. Each of the 3 SOFM parameter we randomly sample from the uniform distribution. Here are 16 randomly generated patterns.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-16-generated-patterns.png"><img alt="16 randomly generated patterns with SOFM network" src="_images/sofm-16-generated-patterns.png" style="width: 100%;"/></a>
</div>
<p>You can see that even small changes in some of the parameters can produce very different results. One of the patterns even look like a bird.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-bird-pattern.png"><img alt="bird pattern generated by SOFM" src="_images/sofm-bird-pattern.png" style="width: 50%;"/></a>
</div>
</div>
<div class="section" id="applications">
<h2>Applications</h2>
<p>It looks a bit strange to think about this approach in context of practical applications, but there are some things that you can do with these patterns. For instance, it can be used to generate unique avatar images for the new website users. Adding more rules and variables to the image generating function we can make patterns even more diverse.</p>
<p>I also tried to play a game with these patterns in order to train my intuition. We use SOFM network parameters in order to generate these image, but reverse procedure is also possible. Seeing the generated pattern we can guess parameters that was used to generate it. For instance, here is a simple example of the pattern that looks like star</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-star-pattern.png"><img alt="start pattern generated by SOFM" src="_images/sofm-star-pattern.png" style="width: 50%;"/></a>
</div>
<p>We can see that only one dot moved in each direction, so we can say that learning radius was zero. Since updated neurons moved far away from the initial grid we can say that learning rate was large.</p>
<p>Here is another one, more complicated.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-blob-pattern.png"><img alt="blob pattern generated by SOFM" src="_images/sofm-blob-pattern.png" style="width: 50%;"/></a>
</div>
<p>In this example it’s harder to say what happened, but we have at least one clue.In the center of the grid there are three dots, arranged horizontally, that didn’t move after all updates. Knowing that there are nine dots in the row, we can conclude that learning radius was 2 or 3. We not sure which one, but if you see that there are just 5 dots left on their initial positions then it’s very likely that radius was 3 since we moved more neurons into new positions.</p>
<p>I hope you got the idea. It’s not always possible to guess the exact values, but each pattern can reveal some clues about the algorithm’s set up.</p>
</div>
<div class="section" id="further-reading">
<h2>Further reading</h2>
<p>If you want to learn more about SOFM, you can <a class="reference internal" href="2017/12/09/sofm_applications.html#sofm-applications"><span>check article</span></a> that covers basic ideas behind SOFM and some of the problems that can be solved with this algorithm.</p>
</div>
<div class="section" id="code">
<h2>Code</h2>
<p>All the code that was used to generate images in the article you can find in <a class="reference external" href="https://github.com/itdxer/neupy/blob/master/notebooks/sofm/The%20Art%20of%20SOFM.ipynb">iPython notebook on github</a>.</p>
</div>

        </div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Yurii Shevchuk</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="tags/sofm.html">sofm</a>, <a href="tags/unsupervised.html">unsupervised</a>, <a href="tags/visualization.html">visualization</a>, <a href="tags/art.html">art</a></span>
        </div>
        <div class="comments">
            <a href="http://neupy.com/2017/12/13/sofm_art.html#disqus_thread" data-disqus-identifier="2017/12/13/sofm_art">Leave a comment</a>
        </div></div><div class="separator post_separator"></div><div class="timestamp postmeta">
            <span>December 09, 2017</span>
        </div>
        <div class="section">
            <span id="sofm-applications"/><h1><a href="2017/12/09/sofm_applications.html"><a class="toc-backref" href="#id1">Self-Organizing Maps and Applications</a></a></h1>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="2017/12/09/sofm_applications.html#self-organizing-maps-and-applications" id="id1">Self-Organizing Maps and Applications</a><ul>
<li><a class="reference internal" href="2017/12/09/sofm_applications.html#introduction" id="id2">Introduction</a></li>
<li><a class="reference internal" href="2017/12/09/sofm_applications.html#intuition-behind-sofm" id="id3">Intuition behind SOFM</a></li>
<li><a class="reference internal" href="2017/12/09/sofm_applications.html#applications" id="id4">Applications</a><ul>
<li><a class="reference internal" href="2017/12/09/sofm_applications.html#clustering" id="id5">Clustering</a></li>
<li><a class="reference internal" href="2017/12/09/sofm_applications.html#space-approximation" id="id6">Space approximation</a></li>
<li><a class="reference internal" href="2017/12/09/sofm_applications.html#high-dimensional-data-visualization" id="id7">High-dimensional data visualization</a></li>
</ul>
</li>
<li><a class="reference internal" href="2017/12/09/sofm_applications.html#visualize-pre-trained-vgg19-network" id="id8">Visualize pre-trained VGG19 network</a></li>
<li><a class="reference internal" href="2017/12/09/sofm_applications.html#summary" id="id9">Summary</a></li>
<li><a class="reference internal" href="2017/12/09/sofm_applications.html#code" id="id10">Code</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="introduction">
<h2><a class="toc-backref" href="#id2">Introduction</a></h2>
<p>I was facinated for a while with SOFM algorithm and that’s why I decided to write this article. The reason why I like it so much because it’s pretty simple and powerful approach that can be applied to solve different problems.</p>
<p>I belive that it’s important to understand the idea behind any algorithm even if you don’t know how to build it. For this reason, I won’t give detailed explanation on how to build your own SOFM network. Instead, I will focus on the intuition behind this algorithm and applications where you can use it. If you want to build it yourself I recommend you to read <a class="reference external" href="http://hagan.okstate.edu/NNDesign.pdf">Neural Network Design</a> book.</p>
</div>
<div class="section" id="intuition-behind-sofm">
<h2><a class="toc-backref" href="#id3">Intuition behind SOFM</a></h2>
<p>As in case of any neural network algorithms the main building blocks for SOFM are <strong>neurons</strong>. Each neuron typically connected to some other neurons, but number of this connections is small. Each neuron connected just to a few other neurons that we call <strong>close neighbours</strong>. There are many ways to arrange these connections, but the most common one is to arrange them into two-dimensional grid.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-grid.png"><img alt="SOFM grid" src="_images/sofm-grid.png" style="width: 50%;"/></a>
</div>
<p>Each blue dot in the image is neuron and line between two neurons means that they are connected. We call this arrangement of neurons <em>grid</em>.</p>
<p>Each neuron in the grid has two properties: position and connections to other neurons. We define connections before we start network training and position is the only thing that changes during the training. There are many ways to initialize position for the neurons, but the easiest one is just to do it randomly. After this initialization grid won’t look as nice as it looks on the image above, but with more training iteration problem can be solved.</p>
<p>Let’s talk about training. In each training iteration we introduce some data point and we try to find neuron that closest to this point. Neuron that closest to this point we call <strong>neuron winner</strong>. But, instead of updating position of this neuron we find its <strong>neighbours</strong>. Note, that it’s not the same as closest neighbours. Before traning we specify special parameter known as <strong>learning radius</strong>. It defines the radius within which we consider other neuron as a neighbours. On the image below you can see the same grid as before with neuron in center that we marked as a winner. You can see in the pictures that larger radius includes more neurons.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-learning-radius-comparison.png"><img alt="Compare SOFM learning radius size" src="_images/sofm-learning-radius-comparison.png" style="width: 100%;"/></a>
</div>
<p>And at the end of the iteration we update our neuron winner and its neighbours positions. We change their position by pushing closer to the data point that we used to find neuron winner. We “push” winner neuron much closer to the data point compared to the neighbour neurons. In fact, the further the nighbours the less “push” it get’s towards the data point. You can see how we update neurons on the image below with different learning radius parameters.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-training-learning-radius-comparison.png"><img alt="Compare SOFM learning radius size" src="_images/sofm-training-learning-radius-comparison.png" style="width: 100%;"/></a>
</div>
<p>You probably noticed that idea is very similar to k-means algorithm, but what makes it really special is the existing relations with other neurons.</p>
<p>It’s easy to compare this algorithm to real world. Imagine that you try to put large tablecloth on the large table. First you put it so that it will partialy cover table. Then you will go around and pull different sides of the tablecloth until you cover the table. But when you pull one side, another part of the tablecloth starts moving to the direction in which you pull it, just like it happens during the training in SOFM.</p>
</div>
<div class="section" id="applications">
<h2><a class="toc-backref" href="#id4">Applications</a></h2>
<p>Surprisingly, this simple idea has a variety of applications. In this part of the article, I’ll cover a few most common applications.</p>
<div class="section" id="clustering">
<h3><a class="toc-backref" href="#id5">Clustering</a></h3>
<p>Clustering is probably the most trivial application where you can use SOFM. In case of clustering, we treat every neuron as a centre of separate cluster. One of the problems is that during the training procedure when we pull one neuron closer to one of the cluster we will be forced to pull its neighbours as well. In order to avoid this issue, we need to break relations between neighbours, so that any update will not have influence on other neurons. If we set up this value as 0 it will mean that neuron winner doesn’t have any relations with other neurons which is exactly what we need for clustering.</p>
<p>In the image below you can see visualized two features from the iris dataset and there are three SOFM neurons colored in grey. As you can see it managed to find pretty good centres of the clusters.</p>
<div class="highlight-bash"><div class="highlight"><pre><span/>$ python sofm_iris_clustering.py
</pre></div>
</div>
<br/><div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-iris-clustering.png"><img alt="Clustering iris dataset using SOFM" src="_images/sofm-iris-clustering.png" style="width: 100%;"/></a>
</div>
<p>Clustering application is the useful one, but it’s not very special one. If you try to run k-mean algorithm on the same dataset that I used in this example you should be able to get roughly the same result. I don’t see any advantages for SOFM with learning radius equal to 0 against k-means. I like to think about SOFM clustering application more like a debugging. When you are trying to find where your code breaks you can disable some parts of it and try to see if the specific function breaks. With SOFM we are disabling some parts in order to see how other things will behave without it.</p>
<p>What would happen if we increase number of clusters? Let’s increase number of clusters from 3 to 20 and run clustering on the same data.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-20-clusters.png"><img alt="Clustering iris dataset using SOFM with 20 clusters" src="_images/sofm-20-clusters.png" style="width: 100%;"/></a>
</div>
<p>Neurons just spread out all over the data trying to cover it. Just in this case, since we have lots of clusters each one will cover smaller portion of the data. We can call it a <strong>micro-clustering</strong>.</p>
</div>
<div class="section" id="space-approximation">
<h3><a class="toc-backref" href="#id6">Space approximation</a></h3>
<p>In the previous example, we tried to do a <strong>space approximation</strong>. Space approximation is similar to clustering, but the goal is here to find the minimum number of points that cover as much data as possible. Since it’s similar to clustering we can use SOFM here as well. But as we saw in the previous example data points wasn’t using space efficiently and some points were very close to each other and some are further. Now the problem is that clusters don’t know about existence of other clusters and they behave independently. To have more cooperative behaviour between clusters we can enable learning radius in SOFM. Let’s try different example. I generated two-dimensional dataset in the shape of the moon that we will try to approximate using SOFM. First, let’s try to do it without increasing learning radius and applying the same micro-clustering technique as before.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-moon-topology-0-radius.png"><img alt="Learning moon topology with clustering" src="_images/sofm-moon-topology-0-radius.png" style="width: 100%;"/></a>
</div>
<p>As you can see we have the same issue as we had with iris dataset. On the left side there are a few cluster centers that very close to each other and on the right side they are further apart. Now, let’s try to set up learning radius equal to 2 and let’s look what will happen.</p>
<div class="highlight-bash"><div class="highlight"><pre><span/>$ python sofm_moon_topology.py
</pre></div>
</div>
<br/><div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-moon-topology.png"><img alt="Learning moon topology with clustering and learning radius" src="_images/sofm-moon-topology.png" style="width: 100%;"/></a>
</div>
<p>You can see that cluster centers are more efficiently distributed along the moon-shaped cluster. Even if we remove data points from the plot the center cluster will give us good understanding about the shape of our original data.</p>
<p>You might ask, what is the use of this application? One of the things that you can do is to use this approach in order to minimize the size of your data sample. The idea is that since feature map spreads out all over the space you can generate smaller dataset that will keep useful properties of the main one. It can be not only useful for training sample minimization, but also for other applications. For instance, in case if you have lots of unlabelled data and labelling can get expensive, you can use the same technique to find smaller sub-sample of the main dataset and label only this subset instead of the random sample.</p>
<p>We can use more than one-dimensional grids in SOFM in order to be able to capture more complicated patterns. In the following example, you can see SOFM with two-dimensional feature map that approximates roughly 8,000 data points using only 100 features.</p>
<div class="highlight-bash"><div class="highlight"><pre><span/>$ python sofm_compare_grid_types.py
</pre></div>
</div>
<br/><div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-grid-types.png"><img alt="Compare hexagonal and rectangular grid types in SOFM" src="_images/sofm-grid-types.png" style="width: 100%;"/></a>
</div>
<p>The same property of space approximation can be extended to the high-dimensional datasets and used for visualizations.</p>
</div>
<div class="section" id="high-dimensional-data-visualization">
<h3><a class="toc-backref" href="#id7">High-dimensional data visualization</a></h3>
<p>We used SOFM with two-dimensional feature map in order to catch dimensional properties of the datasets with only two features. If we increase number of dimensions to three it still would be possible to visualize the result, but in four dimensions it will become a bit trickier.</p>
<p>If we use two-dimensional grid and train SOFM over the high-dimensional data then we can encode network as a heat map where each neuron in the network will be represented by the average distance to its neighbours.</p>
<p>As the example, let’s take a look at the <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html">breast cancer dataset</a> available in the <a class="reference external" href="http://scikit-learn.org">scikit-learn library</a>. This dataset has 30 features and two classes.</p>
<p>Let’s look what we can get if we apply described method on the 30-dimensional data.</p>
<div class="highlight-bash"><div class="highlight"><pre><span/>$ python sofm_heatmap_visualization.py
</pre></div>
</div>
<br/><div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-heatmap.png"><img alt="Embedded 30-dimensional dataset using SOFM" src="_images/sofm-heatmap.png" style="width: 100%;"/></a>
</div>
<p>For this example, I used SOFM with 20x20 feature map. Which basically means that we have 400 micro-clusters. Most of the micro-clusters has either blue squares or red circles and just a few of them has both or none of the classes.</p>
<p>You can see how micro-clusters with blue squares are tended to be close to each other, and the same true for red circles. In fact, we can even draw simple bound that will separate two different classes from each other. Along this bound we can see some cases where micro-cluster has red and blue classes which means that at some places these samples sit very tight. In other cases, like in the left down corner, we can see parts that do not belong to any of the classes which means that there is a gap between data points.</p>
<p>You can also notice that each cell in the heat map has different color. From the colorbar, we can see that black color encodes small numbers and white color encodes large numbers. Each cell has a number associated with it that defines average distance to neighbour clusters. The white color means that cluster is far away from it’s neighbours. Group of the red circles on the right side of the plot has white color, which means that this group is far from the main cluster.</p>
<p>One problem is that color depends on the average distance which can be misleading in some cases. We can build a bit different visualization that will encode distance between two separate micro-clusters as a single value.</p>
<div class="highlight-bash"><div class="highlight"><pre><span/>$ python sofm_heatmap_visualization.py --expanded-heatmap
</pre></div>
</div>
<br/><div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-heatmap-expanded.png"><img alt="Embedded 30-dimensional dataset using SOFM" src="_images/sofm-heatmap-expanded.png" style="width: 100%;"/></a>
</div>
<p>Now between every feature and its neighbour there is an extra square. As in the previous example each square encodes distance between two neighbouring features. We do not consider two features in the map as neighbours in case if they connected diagonally. That’s why all diagonal squares between two micro-clusters color in black. Diagonals are a bit more difficult to encode, because in this case we have two different cases. In order to visualize it we can also take an average of these distances.</p>
<p>More interesting way to make this type of visualization can be with the use of images. In previous case, we use markers to encode two different classes. With images, we can use them directly as the way to represent the cluster. Let’s try to apply this idea on small dataset with images of digits from 0 to 9.</p>
<div class="highlight-bash"><div class="highlight"><pre><span/>$ python sofm_digits.py
</pre></div>
</div>
<br/><div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-digits.png"><img alt="Embeding digit images into two dimensional space using SOFM" src="_images/sofm-digits.png" style="width: 100%;"/></a>
</div>
</div>
</div>
<div class="section" id="visualize-pre-trained-vgg19-network">
<h2><a class="toc-backref" href="#id8">Visualize pre-trained VGG19 network</a></h2>
<p>Using the same techniques, we can look inside the deep neural networks. In this section, I will be looking on the pre-trained VGG19 network using ImageNet data. Only in this case, I decided to make it a bit more challenging. Instead of using data from ImageNet I decided to pick 9 classes of different animal species from <a class="reference external" href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">Caltech 101 dataset</a>. The interesting part is that there are a few species that are not in the ImageNet.</p>
<p>The goal for this visualization is not only to see how the VGG19 network will separate different classes, but also to see if it would be able to extract some special features of the new classes that it hasn’t seen before. This information can be useful for the Transfer Learning, because from the visualization we should be able to see if network can separate unknown class from the other. If it will then it means there is no need to re-train all layers below the one which we are visualizing.</p>
<p>From the Caltech 101 dataset I picked the following classes:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/caltech-101-animal-classes.png"><img alt="9 animal classes from the Caltech 101 dataset" src="_images/caltech-101-animal-classes.png" style="width: 100%;"/></a>
</div>
<p>There are a few classes that hasn’t been used in ImageNet, namely Okapi, Wild cat and Platypus.</p>
<p>Data was prepared in the same way as it was done for the VGG19 during training on ImageNet data. I first removed final layer from the network. Now output for each image should be 4096-dimensional vector. Because of the large dimensional size, I used cosine similarity in order to find closest SOFM neurons (instead of euclidian which we used in all previous examples).</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/vgg19-sofm-dense-2-20x20.png"><img alt="Visualized feature space using pre-trained VGG19 and 9 animal classes from the Caltech 101 dataset" src="_images/vgg19-sofm-dense-2-20x20.png" style="width: 100%;"/></a>
</div>
<p>Even without getting into the details it’s easy to see that SOFM produces pretty meaningful visualization. Similar species are close to each other in the visualization which means that the main properties was captured correctly.</p>
<p>We can also visualize output from the last layer. From the network, we only need to remove final Softmax layer in order to get raw activation values. Using this values, we can also visualize our data.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/vgg19-sofm.png"><img alt="Visualized feature space using pre-trained VGG19 and 9 animal classes from the Caltech 101 dataset" src="_images/vgg19-sofm.png" style="width: 100%;"/></a>
</div>
<p>SOFM managed to identify high-dimensional structure pretty good. There are many interesting things that we can gain from this image. For instance, beaver and platypus share similar features. Since platypus wasn’t a part of the ImageNet dataset it is a reasonable mistake for the network to mix these species.</p>
<p>You probably noticed that there are many black squares in the image. Each square represents a gap between two micro-clusters. You can see how images of separate species are separated from other species with these gaps.</p>
<p>You can also see that network learned to classify rotated and scaled images very similarly which tells us that it is robust against small transformations applied to the image. In the image below, we can see a few examples.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/vgg19-sofm-similar-examples.png"><img alt="Similar images tend to be closer to each other in high-dimensional space" src="_images/vgg19-sofm-similar-examples.png" style="width: 100%;"/></a>
</div>
<p>There are also some things that shows us problems with VGG19 network.. For instance, look at the image of llama that really close to the cheetah’s images.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/vgg19-sofm-llama-similar-to-cheetah.png"><img alt="Llama close to cheetah in high dimensional space." src="_images/vgg19-sofm-llama-similar-to-cheetah.png" style="width: 100%;"/></a>
</div>
<p>This image looks out of place. We can check top 5 classes based on the probability that network gives to this image.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/llama-with-spots.jpg"><img alt="Llama with spots" src="_images/llama-with-spots.jpg" style="width: 30%;"/></a>
</div>
<br/><div class="highlight-python"><div class="highlight"><pre><span/><span class="n">llama</span>                                    <span class="p">:</span> <span class="mf">31.18</span><span class="o">%</span>
<span class="n">cheetah</span><span class="p">,</span> <span class="n">chetah</span><span class="p">,</span> <span class="n">Acinonyx</span> <span class="n">jubatus</span>        <span class="p">:</span> <span class="mf">22.62</span><span class="o">%</span>
<span class="n">tiger</span><span class="p">,</span> <span class="n">Panthera</span> <span class="n">tigris</span>                   <span class="p">:</span> <span class="mf">8.20</span><span class="o">%</span>
<span class="n">lynx</span><span class="p">,</span> <span class="n">catamount</span>                          <span class="p">:</span> <span class="mf">7.34</span><span class="o">%</span>
<span class="n">snow</span> <span class="n">leopard</span><span class="p">,</span> <span class="n">ounce</span><span class="p">,</span> <span class="n">Panthera</span> <span class="n">uncia</span>      <span class="p">:</span> <span class="mf">5.91</span><span class="o">%</span>
</pre></div>
</div>
<p>Prediction is correct, but look at the second choice. Percentage that it might be a cheetah is also pretty high. Even though cheetah and llama species are not very similar to each other, network still thinks that it can be a cheetah. The most obvious explanation of this phenomena is that llama in the image covered with spots all over the body which is a typical feature for cheetah. This example shows how easily we can fool the network.</p>
</div>
<div class="section" id="summary">
<h2><a class="toc-backref" href="#id9">Summary</a></h2>
<p>In the article, I mentioned a few applications where SOFM can be used, but it’s not the full list. It can be also used for other applications like robotics or even for creating some beautiful pictures. It is fascinating how such a simple set of rules can be applied in order to solve very different problems.</p>
<p>Despite all the positive things that can be said about SOFM there are some problems that you encounter.</p>
<ul class="simple">
<li>There are many hyperparameters and selecting the right set of parameter can be tricky.</li>
<li>SOFM doesn’t cover borders of the dataspace which means that area, volume or hypervolume of the data will be smaller than it is in real life. You can see it from the picture where we approximate circles.</li>
</ul>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/sofm-grid-types.png"><img alt="Compare hexagonal and rectangular grid types in SOFM" src="_images/sofm-grid-types.png" style="width: 100%;"/></a>
</div>
<p>It also means that if you need to pick information about outliers from your data - SOFM will probably miss it.</p>
<ul class="simple">
<li>Not every space approximates with SOFM. There can be some cases where SOFM fits data poorly which sometimes difficult to see.</li>
</ul>
</div>
<div class="section" id="code">
<h2><a class="toc-backref" href="#id10">Code</a></h2>
<p>iPython notebook with code that explores VGG19 using SOFM available on <a class="reference external" href="https://github.com/itdxer/neupy/blob/master/notebooks/Looking%20inside%20of%20the%20VGG19%20using%20SOFM.ipynb">github</a>. NeuPy has Python scripts that can help you to start work with SOFM or show you how you can use SOFM for different applications.</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/competitive/sofm_basic.py">Simple SOFM example</a></li>
<li><a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/competitive/sofm_iris_clustering.py">Clustering iris dataset using SOFM</a></li>
<li><a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/competitive/sofm_moon_topology.py">Learning half-circle topology with SOFM</a></li>
<li><a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/competitive/sofm_compare_grid_types.py">Compare feature grid types for SOFM</a></li>
<li><a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/competitive/sofm_compare_weight_init.py">Compare weight initialization methods for SOFM</a></li>
<li><a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/competitive/sofm_digits.py">Visualize digit images in 2D space with SOFM</a></li>
<li><a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/competitive/sofm_heatmap_visualization.py">Embedding 30-dimensional dataset into 2D and building heatmap visualization for SOFM</a></li>
</ul>
</div>

        </div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Yurii Shevchuk</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="tags/sofm.html">sofm</a>, <a href="tags/deep_learning.html">deep learning</a>, <a href="tags/image_recognition.html">image recognition</a>, <a href="tags/unsupervised.html">unsupervised</a>, <a href="tags/visualization.html">visualization</a>, <a href="tags/clustering.html">clustering</a></span>
        </div>
        <div class="comments">
            <a href="http://neupy.com/2017/12/09/sofm_applications.html#disqus_thread" data-disqus-identifier="2017/12/09/sofm_applications">Leave a comment</a>
        </div></div><div class="separator post_separator"></div><div class="timestamp postmeta">
            <span>December 17, 2016</span>
        </div>
        <div class="section">
            <h1><a href="2016/12/17/hyperparameter_optimization_for_neural_networks.html"><a class="toc-backref" href="#id13">Hyperparameter optimization for Neural Networks</a></a></h1>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="2016/12/17/hyperparameter_optimization_for_neural_networks.html#hyperparameter-optimization-for-neural-networks" id="id13">Hyperparameter optimization for Neural Networks</a><ul>
<li><a class="reference internal" href="2016/12/17/hyperparameter_optimization_for_neural_networks.html#introduction" id="id14">Introduction</a></li>
<li><a class="reference internal" href="2016/12/17/hyperparameter_optimization_for_neural_networks.html#hyperparameter-optimization" id="id15">Hyperparameter optimization</a></li>
<li><a class="reference internal" href="2016/12/17/hyperparameter_optimization_for_neural_networks.html#grid-search" id="id16">Grid Search</a></li>
<li><a class="reference internal" href="2016/12/17/hyperparameter_optimization_for_neural_networks.html#random-search" id="id17">Random Search</a></li>
<li><a class="reference internal" href="2016/12/17/hyperparameter_optimization_for_neural_networks.html#hand-tuning" id="id18">Hand-tuning</a></li>
<li><a class="reference internal" href="2016/12/17/hyperparameter_optimization_for_neural_networks.html#bayesian-optimization" id="id19">Bayesian Optimization</a><ul>
<li><a class="reference internal" href="2016/12/17/hyperparameter_optimization_for_neural_networks.html#gaussian-process" id="id20">Gaussian Process</a></li>
<li><a class="reference internal" href="2016/12/17/hyperparameter_optimization_for_neural_networks.html#acquisition-function" id="id21">Acquisition Function</a></li>
<li><a class="reference internal" href="2016/12/17/hyperparameter_optimization_for_neural_networks.html#find-number-of-hidden-units" id="id22">Find number of hidden units</a></li>
<li><a class="reference internal" href="2016/12/17/hyperparameter_optimization_for_neural_networks.html#disadvantages-of-gp-with-ei" id="id23">Disadvantages of GP with EI</a></li>
</ul>
</li>
<li><a class="reference internal" href="2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe" id="id24">Tree-structured Parzen Estimators (TPE)</a><ul>
<li><a class="reference internal" href="2016/12/17/hyperparameter_optimization_for_neural_networks.html#overview" id="id25">Overview</a></li>
<li><a class="reference internal" href="2016/12/17/hyperparameter_optimization_for_neural_networks.html#hyperparameter-optimization-for-mnist-dataset" id="id26">Hyperparameter optimization for MNIST dataset</a></li>
<li><a class="reference internal" href="2016/12/17/hyperparameter_optimization_for_neural_networks.html#disadvantages-of-tpe" id="id27">Disadvantages of TPE</a></li>
</ul>
</li>
<li><a class="reference internal" href="2016/12/17/hyperparameter_optimization_for_neural_networks.html#summary" id="id28">Summary</a></li>
<li><a class="reference internal" href="2016/12/17/hyperparameter_optimization_for_neural_networks.html#source-code" id="id29">Source Code</a></li>
<li><a class="reference internal" href="2016/12/17/hyperparameter_optimization_for_neural_networks.html#references" id="id30">References</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="introduction">
<h2><a class="toc-backref" href="#id14">Introduction</a></h2>
<p>Sometimes it can be difficult to choose a correct architecture for Neural Networks. Usually, this process requires a lot of experience because networks include many parameters. Let’s check some of the most important parameters that we can optimize for the neural network:</p>
<ul class="simple">
<li>Number of layers</li>
<li>Different parameters for each layer (number of hidden units, filter size for convolutional layer and so on)</li>
<li>Type of activation functions</li>
<li>Parameter initialization method</li>
<li>Learning rate</li>
<li>Loss function</li>
</ul>
<p>Even though the list of parameters in not even close to being complete, it’s still impressive how many parameters influences network’s accuracy.</p>
</div>
<div class="section" id="hyperparameter-optimization">
<h2><a class="toc-backref" href="#id15">Hyperparameter optimization</a></h2>
<p>In this article, I would like to show a few different hyperparameter selection methods.</p>
<ul class="simple">
<li>Grid Search</li>
<li>Random Search</li>
<li>Hand-tuning</li>
<li>Gaussian Process with Expected Improvement</li>
<li>Tree-structured Parzen Estimators (TPE)</li>
</ul>
</div>
<div class="section" id="grid-search">
<h2><a class="toc-backref" href="#id16">Grid Search</a></h2>
<p>The simplest algorithms that you can use for hyperparameter optimization is a Grid Search. The idea is simple and straightforward. You just need to define a set of parameter values, train model for all possible parameter combinations and select the best one. This method is a good choice only when model can train quickly, which is not the case for typical neural networks.</p>
<p>Imagine that we need to optimize 5 parameters. Let’s assume, for simplicity, that we want to try 10 different values per each parameter. Therefore, we need to make 100,000 (<span class="math">\(10 ^ 5\)</span>) evaluations. Assuming that network trains 10 minutes on average we will have finished hyperparameter tuning in almost 2 years. Seems crazy, right? Typically, network trains much longer and we need to tune more hyperparameters, which means that it can take forever to run grid search for typical neural network. The better solution is random search.</p>
</div>
<div class="section" id="random-search">
<h2><a class="toc-backref" href="#id17">Random Search</a></h2>
<p>The idea is similar to Grid Search, but instead of trying all possible combinations we will just use randomly selected subset of the parameters. Instead of trying to check 100,000 samples we can check only 1,000 of parameters. Now it should take a week to run hyperparameter optimization instead of 2 years.</p>
<p>Let’s sample 100 two-dimensional data points from a uniform distribution.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/100-uniform-data-points.png"><img alt="Randomly generated 100 data points" src="_images/100-uniform-data-points.png" style="width: 100%;"/></a>
</div>
<p>In case if there are not enough data points, random sampling doesn’t fully covers parameter space. It can be seen in the figure above because there are some regions that don’t have data points. In addition, it samples some points very close to each other which are redundant for our purposes. We can solve this problem with <a class="reference external" href="https://en.wikipedia.org/wiki/Low-discrepancy_sequence">Low-discrepancy sequences</a> (also called quasi-random sequences).</p>
<p>There are many different techniques for quasi-random sequences:</p>
<ul class="simple">
<li>Sobol sequence</li>
<li>Hammersley set</li>
<li>Halton sequence</li>
<li>Poisson disk sampling</li>
</ul>
<p>Let’s compare some of the mentioned methods with previously random sampled data points.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/100-random-points.png"><img alt="Randomly generated 100 data points" src="_images/100-random-points.png" style="width: 100%;"/></a>
</div>
<p>As we can see now sampled points spread out through the parameter space more uniformly. One disadvantage of these methods is that not all of them can provide you good results for the higher dimensions. For instance, Halton sequence and Hammersley set do not work well for dimension bigger than 10 <a class="footnote-reference" href="#id11" id="id1">[7]</a>.</p>
<p>Even though we improved hyperparameter optimization algorithm it still is not suitable for large neural networks.</p>
<p>But before we move on to more complicated methods I want to focus on parameter hand-tuning.</p>
</div>
<div class="section" id="hand-tuning">
<h2><a class="toc-backref" href="#id18">Hand-tuning</a></h2>
<p>Let’s start with an example. Imagine that we want to select the best number of units in the hidden layer (we set up just one hyperparameter for simplicity). The simplest thing is to try different values and select the best one. Let’s say we set up 10 units for the hidden layer and train the network. After the training, we check the accuracy for the validation dataset and it turns out that we classified 65% of the samples correctly.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/hid-units-vs-accuracy-iter1.png"><img alt="Hidden units vs Accuracy, Iteration #1" src="_images/hid-units-vs-accuracy-iter1.png" style="width: 100%;"/></a>
</div>
<p>The accuracy is low, so it’s intuitive to think that we need more units in a hidden layer. Let’s increase the number of units and check the improvement. But, by how many should we increase the number of units? Will small changes make a significant effect on the prediction accuracy? Would it be a good step to set up a number of hidden units equal to 12? Probably not. So let’s go further and explore parameters from the next order of magnitude. We can set up a number of hidden units equal to 100.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/hid-units-vs-accuracy-iter2.png"><img alt="Hidden units vs Accuracy, Iteration #2" src="_images/hid-units-vs-accuracy-iter2.png" style="width: 100%;"/></a>
</div>
<p>For the 100 hidden units, we got prediction accuracy equal to 82% which is a great improvement compared to 65%. Two points in the figure above show us that by increasing number of hidden units we increase the accuracy. We can proceed using the same strategy and train network with 200 hidden units.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/hid-units-vs-accuracy-iter3.png"><img alt="Hidden units vs Accuracy, Iteration #3" src="_images/hid-units-vs-accuracy-iter3.png" style="width: 100%;"/></a>
</div>
<p>After the third iteration, our prediction accuracy is 84%. We’ve increased the number of units by a factor of two and got only 2% of improvement.</p>
<p>We can keep going, but I think judging by this example it is clear that human can select parameters better than Grid search or Random search algorithms. The main reason why is that we are able to learn from our previous mistakes. After each iteration, we memorize and analyze our previous results. This information gives us a much better way for selection of the next set of parameters. And even more than that. The more you work with neural networks the better intuition you develop for what and when to use.</p>
<p>Nevertheless, let’s get back to our optimization problem. How can we automate the process described above? One way of doing this is to apply a Bayesian Optimization.</p>
</div>
<div class="section" id="bayesian-optimization">
<h2><a class="toc-backref" href="#id19">Bayesian Optimization</a></h2>
<p>Bayesian optimization is a derivative-free optimization method. There are a few different algorithm for this type of optimization, but I was specifically interested in Gaussian Process with Acquisition Function. For some people it can resemble the method that we’ve described above in the Hand-tuning section. Gaussian Process uses a set of previously evaluated parameters and resulting accuracy to make an assumption about unobserved parameters. Acquisition Function using this information suggest the next set of parameters.</p>
<div class="section" id="gaussian-process">
<h3><a class="toc-backref" href="#id20">Gaussian Process</a></h3>
<p>The idea behind Gaussian Process is that for every input <span class="math">\(x\)</span> we have output <span class="math">\(y = f(x)\)</span>, where <span class="math">\(f\)</span> is a stochastic function. This function samples output from a gaussian distribution. Also, we can say that each input <span class="math">\(x\)</span> has associated gaussian distribution. Which means that for each input <span class="math">\(x\)</span> gaussian process has defined mean  <span class="math">\(\mu\)</span> and standard deviation <span class="math">\(\sigma\)</span> for some gaussian distribution.</p>
<p>Gaussian Process is a generalization of <a class="reference external" href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">Multivariate Gaussian Distribution</a>. Multivariate Gaussian Distribution is defined by mean vector and covariance matrix, while Gaussian Process is defined by mean function and covariance function. Basically, a function is an infinite vector. Also, we can say that Multivariate Gaussian Distribution is a Gaussian Process for the functions with a discrete number of possible inputs.</p>
<p>I always like to have some picture that shows me a visual description of an algorithm. One of such visualizations of the Gaussian Process I found in the Introduction to Gaussian Process slides <a class="footnote-reference" href="#id7" id="id2">[3]</a>.</p>
<p>Let’s check some Multivariate Gaussian Distribution defined by mean vector <span class="math">\(\mu\)</span></p>
<div class="math">
\[\begin{split}\begin{align*}
    \mu =
    \left[
    \begin{array}{c}
      0.0 &amp; 1.0 \\
    \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>and covariance matrix <span class="math">\(\Sigma\)</span></p>
<div class="math">
\[\begin{split}\begin{align*}
    \Sigma =
    \left[
    \begin{array}{c}
      1.0 &amp; 0.7 \\
      0.7 &amp; 2.5 \\
    \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>We can sample 100 points from this distribution and make a scatter plot.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/mulvar-gauss-dist-example.png"><img alt="Multivariate Gaussian Distribution Example" src="_images/mulvar-gauss-dist-example.png" style="width: 100%;"/></a>
</div>
<p>Another way to visualize these samples might be <a class="reference external" href="https://en.wikipedia.org/wiki/Parallel_coordinates">Parallel Coordinates</a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/mulvar-gauss-dist-parallel-coords.png"><img alt="Multivariate Gaussian Distribution in Parallel Coordinates Example" src="_images/mulvar-gauss-dist-parallel-coords.png" style="width: 100%;"/></a>
</div>
<p>You should understand that lines that connect points are just an imaginary relations between each coordinate. There is nothing in between Random variable #1 and Random variable #2.</p>
<p>An interesting thing happens when we increase the number of samples.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/mulvar-gauss-dist-parallel-coords-many-samples.png"><img alt="Multivariate Gaussian Distribution in Parallel Coordinates Example with 3000 samples" src="_images/mulvar-gauss-dist-parallel-coords-many-samples.png" style="width: 100%;"/></a>
</div>
<p>Now we can see that lines form a smooth shape. This shape defines a correlation between two random variables. If it’s very narrow in the middle then there is a negative correlation between two random variables.</p>
<p>With scatter plot we are limited to numbers of dimensions that we can visualize, but with Parallel Coordinates we can add more dimensions. Let’s define new Multivariate Gaussian Distribution using 5 random variables.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/mulvar-gauss-dist-parallel-coords-5d.png"><img alt="Multivariate Gaussian Distribution in Parallel Coordinates for multiple dimensions" src="_images/mulvar-gauss-dist-parallel-coords-5d.png" style="width: 100%;"/></a>
</div>
<p>With more variables, it looks more like a function. We can increase the number of dimensions and still be able to visualize Multivariate Gaussian Distribution. The more dimensions we add the more it looks like a set of functions sampled from the Gaussian Process. But in case of Gaussian Process number of dimensions should be infinite.</p>
<p>Let’s get data from the Hand-tuning section (the one where with 10 hidden units we got 65% of accuracy). Using this data we can train Gaussian Process and predict mean and standard deviation for each point <span class="math">\(x\)</span>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/gaussian-process-example.png"><img alt="Gaussian Process regression example for second and third iterations" src="_images/gaussian-process-example.png" style="width: 100%;"/></a>
</div>
<p>The blue region defines 95% confidence interval for each point <span class="math">\(x\)</span>. It’s easy to see that the further we go from the observed samples the wider confidence interval becomes which is a logical conclusion. The opposite is true as well. Very similar to the logic that a person uses to select next set of parameters.</p>
<p>From the plot, it looks like observed data points doesn’t have any variance. In fact, the variance is not zero, it’s just really tiny. That’s because our previous Gaussian Process configuration is expecting that our prediction was obtained from a deterministic function which is not true for most neural networks. To fix it we can change the parameter for the Gaussian Process that defines the amount of noise in observed variables. This trick will not only give us a prediction that is less certain but also a mean of the number of hidden units that won’t go through the observed data points.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/gaussian-process-noise-example.png"><img alt="Gaussian Process regression example with noise for second and third iterations" src="_images/gaussian-process-noise-example.png" style="width: 100%;"/></a>
</div>
</div>
<div class="section" id="acquisition-function">
<h3><a class="toc-backref" href="#id21">Acquisition Function</a></h3>
<p>Acquisition Function defines the set of parameter for our next step. There are many different functions <a class="footnote-reference" href="#id5" id="id3">[1]</a> that can help us calculate the best value for the next step. One of the most common is Expected Improvement. There are two ways to compute it. In case if we are trying to find minimum we can use this formula.</p>
<div class="math">
\[g_{min}(x) = max(0, y_{min} - y_{lowest\ expected})\]</div>
<p>where <span class="math">\(y_{min}\)</span> is the minimum observed value <span class="math">\(y\)</span> and <span class="math">\(y_{lowest\ expected}\)</span> lowest possible value from the confidence interval associated with each possible value <span class="math">\(x\)</span>.</p>
<p>In our case, we are trying to find the maximum value. With the small modifications, we can change last formula in the way that will identify Expected Improvement for the maximum value.</p>
<div class="math">
\[g_{max}(x) = max(0, y_{highest\ expected} - y_{max})\]</div>
<p>where <span class="math">\(y_{max}\)</span> is the maximum observed value and <span class="math">\(y_{highest\ expected}\)</span> highest possible value from the confidence interval associated with each possible value <span class="math">\(x\)</span>.</p>
<p>Here is an output for each point <span class="math">\(x\)</span> for the Expected Improvement function.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/expected-improvement-example.png"><img alt="Expected Improvement example" src="_images/expected-improvement-example.png" style="width: 100%;"/></a>
</div>
</div>
<div class="section" id="find-number-of-hidden-units">
<h3><a class="toc-backref" href="#id22">Find number of hidden units</a></h3>
<p>Let’s try to build a hyperparameter optimizer based on Gaussian Process regression and Expected Improvement function. We will continue work with the previous problem where we tried to find the best number of hidden units. But for this time we will try to create a network for digit classification tasks.</p>
<p>Let’s define a function that trains the neural network and return prediction error.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span><span class="p">,</span> <span class="n">layers</span>

<span class="k">def</span> <span class="nf">train_network</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    <span class="n">network</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="mi">64</span><span class="p">),</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">Relu</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">),</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
        <span class="p">],</span>

        <span class="c1"># Randomly shuffle dataset before each</span>
        <span class="c1"># training epoch.</span>
        <span class="n">shuffle_data</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>

        <span class="c1"># Do not show training progress in output</span>
        <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>

        <span class="n">step</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
        <span class="n">error</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">network</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

    <span class="c1"># Calculates categorical cross-entropy error between</span>
    <span class="c1"># predicted value for x_test and y_test value</span>
    <span class="k">return</span> <span class="n">network</span><span class="o">.</span><span class="n">prediction_error</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
<p>Let’s import digits dataset from scikit-learn.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">environment</span>

<span class="n">environment</span><span class="o">.</span><span class="n">reproducible</span><span class="p">()</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">size</span>
<span class="n">n_classes</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># One-hot encoder</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">))</span>
<span class="n">target</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_samples</span><span class="p">),</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.7</span>
<span class="p">)</span>
</pre></div>
</div>
<p>And for the last step, we need to define parameter selection procedure. First, we need to define a function that performs Gaussian Process regression and returns mean and standard deviation of the prediction for the specified input vector.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcess</span>

<span class="k">def</span> <span class="nf">vector_2d</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">array</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">gaussian_process</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">):</span>
    <span class="n">x_train</span> <span class="o">=</span> <span class="n">vector_2d</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">vector_2d</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">x_test</span> <span class="o">=</span> <span class="n">vector_2d</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

    <span class="c1"># Train gaussian process</span>
    <span class="n">gp</span> <span class="o">=</span> <span class="n">GaussianProcess</span><span class="p">(</span><span class="n">corr</span><span class="o">=</span><span class="s1">'squared_exponential'</span><span class="p">,</span>
                         <span class="n">theta0</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">,</span> <span class="n">thetaL</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">thetaU</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">gp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="c1"># Get mean and standard deviation for each possible</span>
    <span class="c1"># number of hidden units</span>
    <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_var</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">eval_MSE</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">y_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">vector_2d</span><span class="p">(</span><span class="n">y_var</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span>
</pre></div>
</div>
<p>Next, we need to apply to the predicted output Expected Improvement (EI) and find out next optimal step.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">next_parameter_by_ei</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span><span class="p">,</span> <span class="n">x_choices</span><span class="p">):</span>
    <span class="c1"># Calculate expecte improvement from 95% confidence interval</span>
    <span class="n">expected_improvement</span> <span class="o">=</span> <span class="n">y_min</span> <span class="o">-</span> <span class="p">(</span><span class="n">y_mean</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">y_std</span><span class="p">)</span>
    <span class="n">expected_improvement</span><span class="p">[</span><span class="n">expected_improvement</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">max_index</span> <span class="o">=</span> <span class="n">expected_improvement</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
    <span class="c1"># Select next choice</span>
    <span class="n">next_parameter</span> <span class="o">=</span> <span class="n">x_choices</span><span class="p">[</span><span class="n">max_index</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">next_parameter</span>
</pre></div>
</div>
<p>And finally, we can override all procedure in one function.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">random</span>

<span class="k">def</span> <span class="nf">hyperparam_selection</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">n_hidden_range</span><span class="p">,</span> <span class="n">func_args</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">func_args</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">func_args</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">min_n_hidden</span><span class="p">,</span> <span class="n">max_n_hidden</span> <span class="o">=</span> <span class="n">n_hidden_range</span>
    <span class="n">n_hidden_choices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">min_n_hidden</span><span class="p">,</span> <span class="n">max_n_hidden</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># To be able to perform gaussian process we need to</span>
    <span class="c1"># have at least 2 samples.</span>
    <span class="n">n_hidden</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">min_n_hidden</span><span class="p">,</span> <span class="n">max_n_hidden</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="o">*</span><span class="n">func_args</span><span class="p">)</span>

    <span class="n">parameters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">)</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

    <span class="n">n_hidden</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">min_n_hidden</span><span class="p">,</span> <span class="n">max_n_hidden</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="o">*</span><span class="n">func_args</span><span class="p">)</span>

        <span class="n">parameters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">)</span>
        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

        <span class="n">y_min</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span> <span class="o">=</span> <span class="n">gaussian_process</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span>
                                         <span class="n">n_hidden_choices</span><span class="p">)</span>

        <span class="n">n_hidden</span> <span class="o">=</span> <span class="n">next_parameter_by_ei</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span><span class="p">,</span>
                                        <span class="n">n_hidden_choices</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">y_min</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">n_hidden</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
            <span class="c1"># Lowest expected improvement value have been achieved</span>
            <span class="k">break</span>

    <span class="n">min_score_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">parameters</span><span class="p">[</span><span class="n">min_score_index</span><span class="p">]</span>
</pre></div>
</div>
<p>Now we are able to run a few iterations and find a number of hidden units that gave better results during the training.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="n">best_n_hidden</span> <span class="o">=</span> <span class="n">hyperparam_selection</span><span class="p">(</span>
    <span class="n">train_network</span><span class="p">,</span>
    <span class="n">n_hidden_range</span><span class="o">=</span><span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1000</span><span class="p">],</span>
    <span class="n">func_args</span><span class="o">=</span><span class="p">[</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">],</span>
    <span class="n">n_iter</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/hyperparam-selection-nn-hidden-units.png"><img alt="Select number of hidden units for Neural Network" src="_images/hyperparam-selection-nn-hidden-units.png" style="width: 100%;"/></a>
</div>
<p>With small modifications, it’s possible to add an additional functionality to the function that allows optimizing more hyperparameters at once.</p>
</div>
<div class="section" id="disadvantages-of-gp-with-ei">
<h3><a class="toc-backref" href="#id23">Disadvantages of GP with EI</a></h3>
<p>There are a few disadvantages related to the Gaussian Process with Expected Improvement.</p>
<ol class="arabic simple">
<li>It doesn’t work well for categorical variables. In case if neural networks it can be a type of activation function.</li>
<li>GP with EI selects new set of parameters based on the best observation. Neural Network usually involves randomization (like weight initialization and dropout) during the training process which influences a final score. Running neural network with the same parameters can lead to different scores. Which means that our best score can be just lucky output for the specific set of parameters.</li>
<li>It can be difficult to select right hyperparameters for Gaussian Process. Gaussian Process has lots of different kernel types. In addition you can construct more complicated kernels using simple kernels as a building block.</li>
<li>It works slower when number of hyperparameters increases. That’s an issue when you deal with a huge number of parameters.</li>
</ol>
</div>
</div>
<div class="section" id="tree-structured-parzen-estimators-tpe">
<h2><a class="toc-backref" href="#id24">Tree-structured Parzen Estimators (TPE)</a></h2>
<div class="section" id="overview">
<h3><a class="toc-backref" href="#id25">Overview</a></h3>
<p>Tree-structured Parzen Estimators (TPE) fixes disadvantages of the Gaussian Process. Each iteration TPE collects new observation and at the end of the iteration, the algorithm decides which set of parameters it should try next. The main idea is similar, but an algorithm is completely different</p>
<p>At the very beginning, we need to define a prior distribution for out hyperparameters. By default, they can be all uniformly distributed, but it’s possible to associate any hyperparameter with some random unimodal distribution.</p>
<p>For the first few iterations, we need to warn up TPE algorithm. It means that we need to collect some data at first before we can apply TPE. The best and simplest way to do it is just to perform a few iterations of Random Search. A number of iterations for Random Search is a parameter defined by the user for the TPE algorithm.</p>
<p>When we collected some data we can finally apply TPE. The next step is to divide collected observations into two groups. The first group contains observations that gave best scores after evaluation and the second one - all other observations. And the goal is to find a set of parameters that more likely to be in the first group and less likely to be in the second group. The fraction of the best observations is defined by the user as a parameter for the TPE algorithm. Typically, it’s 10-25% of observations.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/tpe-observation-groups.png"><img alt="Observation groups for TPE" src="_images/tpe-observation-groups.png" style="width: 100%;"/></a>
</div>
<p>As you can see we are no longer rely on the best observation. Instead, we use a distribution of the best observations. The more iterations we use during the Random Search the better distribution we have at the beginning.</p>
<p>The next part of the TPE is to model likelihood probability for each of the two groups. This is the next big difference between Gaussian Process and TPE. For Gaussian Process we’ve modeled posterior probability instead of likelihood probability. Using the likelihood probability from the first group (the one that contains best observations) we sample the bunch of candidates. From the sampled candidates we try to find a candidate that more likely to be in the first group and less likely to be in the second one. The following formula defines Expected Improvement per each candidate.</p>
<div class="math">
\[EI(x) = \frac{l(x)}{g(x)}\]</div>
<p>Where <span class="math">\(l(x)\)</span> is a probability being in the first group and <span class="math">\(g(x)\)</span> is a probability being in the second group.</p>
<p>Here is an example. Let’s say we have predefined distribution for both groups. From the group #1, we sample 6 candidates. And for each, we calculate Expected Improvement. A parameter that has the highest improvement is the one that we will use for the next iteration.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/tpe-sampled-candidates.png"><img alt="Candidates sampling for TPE" src="_images/tpe-sampled-candidates.png" style="width: 100%;"/></a>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/tpe-expected-improvement.png"><img alt="Expected improvement for TPE" src="_images/tpe-expected-improvement.png" style="width: 100%;"/></a>
</div>
<p>In the example, I’ve used t-distributions, but in TPE distribution models using <a class="reference external" href="https://www.cs.utah.edu/~suyash/Dissertation_html/node11.html">parzen-window density estimators</a>. The main idea is that each sample defines gaussian distribution with specified mean (value of the hyperparameter) and standard deviation. Then all these points stacks together and normalized to assure that output is Probability Density Function (PDF). That’s why <cite>Parzen estimators</cite> appears in the name of the algorithm.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/parzen-estimators.png"><img alt="Parzen estimators" src="_images/parzen-estimators.png" style="width: 100%;"/></a>
</div>
<p>And the <cite>tree-structured</cite> means that parameter space defines in a form of a tree. Later we will try to find the best number of layers for the network. In our case, we will try to decide whether it’s better to use one or two hidden layers. In case if we use two hidden layers we should define the number of hidden units for the first and second layer independently. If we use one hidden layer we don’t need to define the number of hidden units for the second hidden layer, because it doesn’t exist for the specified set of parameter. Basically, it means that a number of hidden units in the second hidden layer depends on the number of hidden layers. Which means that parameters have tree-structured dependencies.</p>
</div>
<div class="section" id="hyperparameter-optimization-for-mnist-dataset">
<h3><a class="toc-backref" href="#id26">Hyperparameter optimization for MNIST dataset</a></h3>
<p>Let’s make an example. We’re going to use MNIST dataset.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">mnist</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_mldata</span><span class="p">(</span><span class="s1">'MNIST original'</span><span class="p">)</span>

<span class="n">target_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">OneHotEncoder</span><span class="p">()</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">target_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">todense</span><span class="p">()</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">data</span> <span class="o">/</span> <span class="mf">255.</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span> <span class="o">-</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="n">target</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="n">train_size</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span> <span class="o">/</span> <span class="mf">7.</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>For hyperparameter selection, I’m going to use <a class="reference external" href="https://github.com/hyperopt/hyperopt">hyperopt</a> library. It has implemented TPE algorithm.</p>
<p>The hyperopt library gives the ability to define a prior distribution for each parameter. In the table below you can find information about parameters that we are going to tune.</p>
<table border="1" class="docutils">
<colgroup>
<col width="33%"/>
<col width="33%"/>
<col width="33%"/>
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Parameter name</th>
<th class="head">Distribution</th>
<th class="head">Values</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Step size</td>
<td>Log-uniform</td>
<td><span class="math">\(x \in [0.01, 0.5]\)</span></td>
</tr>
<tr class="row-odd"><td>Batch size</td>
<td>Log-uniform integer</td>
<td><span class="math">\(x \in [16, 512]\)</span></td>
</tr>
<tr class="row-even"><td>Activation function</td>
<td>Categorical</td>
<td><span class="math">\(x \in \{Relu, PRelu, Elu, Sigmoid, Tanh\}\)</span></td>
</tr>
<tr class="row-odd"><td>Number of hidden layers</td>
<td>Categorical</td>
<td><span class="math">\(x \in \{1, 2\}\)</span></td>
</tr>
<tr class="row-even"><td>Number of units in the first layer</td>
<td>Uniform integer</td>
<td><span class="math">\(x \in [50, 1000]\)</span></td>
</tr>
<tr class="row-odd"><td>Number of units in the second layer (In case if it defined)</td>
<td>Uniform integer</td>
<td><span class="math">\(x \in [50, 1000]\)</span></td>
</tr>
<tr class="row-even"><td>Dropout layer</td>
<td>Uniform</td>
<td><span class="math">\(x \in [0, 0.5]\)</span></td>
</tr>
</tbody>
</table>
<p>Here is one way to define our parameters in hyperopt.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">hyperopt</span> <span class="kn">import</span> <span class="n">hp</span>

<span class="k">def</span> <span class="nf">uniform_int</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">):</span>
    <span class="c1"># `quniform` returns:</span>
    <span class="c1"># round(uniform(low, high) / q) * q</span>
    <span class="k">return</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">loguniform_int</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">):</span>
    <span class="c1"># Do not forget to make a logarithm for the</span>
    <span class="c1"># lower and upper bounds.</span>
    <span class="k">return</span> <span class="n">hp</span><span class="o">.</span><span class="n">qloguniform</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lower</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">upper</span><span class="p">),</span> <span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">parameter_space</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'step'</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="s1">'step'</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
    <span class="s1">'layers'</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">'layers'</span><span class="p">,</span> <span class="p">[{</span>
        <span class="s1">'n_layers'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s1">'n_units_layer'</span><span class="p">:</span> <span class="p">[</span>
            <span class="n">uniform_int</span><span class="p">(</span><span class="s1">'n_units_layer_11'</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span>
        <span class="p">],</span>
    <span class="p">},</span> <span class="p">{</span>
        <span class="s1">'n_layers'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="s1">'n_units_layer'</span><span class="p">:</span> <span class="p">[</span>
            <span class="n">uniform_int</span><span class="p">(</span><span class="s1">'n_units_layer_21'</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span>
            <span class="n">uniform_int</span><span class="p">(</span><span class="s1">'n_units_layer_22'</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span>
        <span class="p">],</span>
    <span class="p">}]),</span>
    <span class="s1">'act_func_type'</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">'act_func_type'</span><span class="p">,</span> <span class="p">[</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Relu</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">PRelu</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Elu</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Tanh</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Sigmoid</span>
    <span class="p">]),</span>

    <span class="s1">'dropout'</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="s1">'dropout'</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
    <span class="s1">'batch_size'</span><span class="p">:</span> <span class="n">loguniform_int</span><span class="p">(</span><span class="s1">'batch_size'</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
<span class="p">}</span>
</pre></div>
</div>
<p>I won’t get into details. I think that definitions are pretty clear from the code. In case if you want to learn more about hyperopt parameter space initialization you can check <a class="reference external" href="https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions">this link</a>.</p>
<p>Next we need to construct a function that we want to minimize. In our case function should train network using training dataset and return cross entropy error for validation dataset.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="k">def</span> <span class="nf">train_network</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"Parameters:"</span><span class="p">)</span>
    <span class="n">pprint</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>
</pre></div>
</div>
<p>First of all, in the training function, we need to extract our parameter.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="n">step</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">'step'</span><span class="p">]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s1">'batch_size'</span><span class="p">])</span>
<span class="n">proba</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">'dropout'</span><span class="p">]</span>
<span class="n">activation_layer</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">'act_func_type'</span><span class="p">]</span>
<span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">'layers'</span><span class="p">][</span><span class="s1">'n_units_layer'</span><span class="p">]]</span>
</pre></div>
</div>
<p>Note that some of the parameters I converted to the integer. The problem is that hyperopt returns float types and we need to convert them.</p>
<p>Next, we need to construct network based on the presented information. In our case, we use only one or two hidden layers, but it can be any arbitrary number of layers.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="mi">784</span><span class="p">)</span>

<span class="k">for</span> <span class="n">layer_size</span> <span class="ow">in</span> <span class="n">layer_sizes</span><span class="p">:</span>
    <span class="n">network</span> <span class="o">=</span> <span class="n">network</span> <span class="o">&gt;</span> <span class="n">activation_layer</span><span class="p">(</span><span class="n">layer_size</span><span class="p">)</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">network</span> <span class="o">&gt;</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">proba</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">layers</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>To learn more about layers in NeuPy you should check <a class="reference internal" href="docs/layers/basics.html#layers-basics"><span>documentation</span></a>.</p>
<p>After that, we can define training algorithm for the network.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span>
<span class="kn">from</span> <span class="nn">neupy.exceptions</span> <span class="kn">import</span> <span class="n">StopTraining</span>

<span class="k">def</span> <span class="nf">on_epoch_end</span><span class="p">(</span><span class="n">network</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">network</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">last</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">StopTraining</span><span class="p">(</span><span class="s2">"Training was interrupted. Error is to high."</span><span class="p">)</span>

<span class="n">mnet</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">RMSProp</span><span class="p">(</span>
    <span class="n">network</span><span class="p">,</span>

    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">,</span>

    <span class="n">error</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
    <span class="n">shuffle_data</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>

    <span class="n">epoch_end_signal</span><span class="o">=</span><span class="n">on_epoch_end</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>All settings should be clear from the code. You can check <a class="reference internal" href="apidocs/neupy.algorithms.gd.rmsprop.html#neupy.algorithms.gd.rmsprop.RMSProp" title="neupy.algorithms.gd.rmsprop.RMSProp"><span class="xref py py-class docutils literal"><span class="pre">RMSProp</span></span></a> documentation to find more information about its input parameters. In addition, I’ve added a simple rule that interrupts training when the error is too high. This is an example of a simple rule that can be changed.</p>
<p>Now we can train our network.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="n">mnet</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
<p>And at the end of the function, we can check some information about the training progress.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="n">score</span> <span class="o">=</span> <span class="n">mnet</span><span class="o">.</span><span class="n">prediction_error</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">mnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">y_predicted</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">"Final score: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Accuracy: {:.2%}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>

<span class="k">return</span> <span class="n">score</span>
</pre></div>
</div>
<p>You can see that I’ve used two evaluation metrics. First one is cross-entropy. NeuPy uses it as a validation error function when we call the <span class="docutils literal"><span class="pre">prediction_error</span></span> method. The second one is just a prediction accuracy.</p>
<p>And finally, we run hyperparameter optimization.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">hyperopt</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="c1"># Object stores all information about each trial.</span>
<span class="c1"># Also, it stores information about the best trial.</span>
<span class="n">trials</span> <span class="o">=</span> <span class="n">hyperopt</span><span class="o">.</span><span class="n">Trials</span><span class="p">()</span>

<span class="n">tpe</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
    <span class="n">hyperopt</span><span class="o">.</span><span class="n">tpe</span><span class="o">.</span><span class="n">suggest</span><span class="p">,</span>

    <span class="c1"># Sample 1000 candidate and select candidate that</span>
    <span class="c1"># has highest Expected Improvement (EI)</span>
    <span class="n">n_EI_candidates</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>

    <span class="c1"># Use 20% of best observations to estimate next</span>
    <span class="c1"># set of parameters</span>
    <span class="n">gamma</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>

    <span class="c1"># First 20 trials are going to be random</span>
    <span class="n">n_startup_jobs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">hyperopt</span><span class="o">.</span><span class="n">fmin</span><span class="p">(</span>
    <span class="n">train_network</span><span class="p">,</span>

    <span class="n">trials</span><span class="o">=</span><span class="n">trials</span><span class="p">,</span>
    <span class="n">space</span><span class="o">=</span><span class="n">parameter_space</span><span class="p">,</span>

    <span class="c1"># Set up TPE for hyperparameter optimization</span>
    <span class="n">algo</span><span class="o">=</span><span class="n">tpe</span><span class="p">,</span>

    <span class="c1"># Maximum number of iterations. Basically it trains at</span>
    <span class="c1"># most 200 networks before selecting the best one.</span>
    <span class="n">max_evals</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>And after all trials, we can check the best one in the <span class="docutils literal"><span class="pre">trials.best_trial</span></span> attribute.</p>
</div>
<div class="section" id="disadvantages-of-tpe">
<h3><a class="toc-backref" href="#id27">Disadvantages of TPE</a></h3>
<p>On of the biggest disadvantages of this algorithm is that it selects parameters independently from each other. For instance, there is a clear relation between regularization and number of training epoch parameters. With regularization, we usually can train network for more epochs and with more epochs we can achieve better results. On the other hand without regularization, many epochs can be a bad choice because network starts overfitting and validation error increases. Without taking into account the state of the regularization variable each next choice for the number of epochs can look arbitrary.</p>
<p>It’s good in case if you now that some variables have relations. To overcome problem from the previous example you can construct two different choices for epochs. The first one will enable regularization and selects a number of epochs from the <span class="math">\([500, 1000]\)</span> range. And the second one without regularization and selects number of epochs from the <span class="math">\([10, 200]\)</span> range.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">'training_parameters'</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s1">'regularization'</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
        <span class="s1">'n_epochs'</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">'n_epochs'</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">},</span> <span class="p">{</span>
        <span class="s1">'regularization'</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
        <span class="s1">'n_epochs'</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">'n_epochs'</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">},</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="summary">
<h2><a class="toc-backref" href="#id28">Summary</a></h2>
<p>The Bayesian Optimization and TPE algorithms show great improvement over the classic hyperparameter optimization methods. They allow to learn from the training history and give better and better estimations for the next set of parameters. But it still takes lots of time to apply these algorithms. It’s great if you have an access to multiple machines and you can parallel parameter tuning procedure <a class="footnote-reference" href="#id8" id="id4">[4]</a>, but usually, it’s not an option. Sometimes it’s better just to avoid hyperparameter optimization. In case if you just try to build a network for trivial problems like image classification it’s better to use existed architectures with pre-trained parameters like <a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/cnn/alexnet.py">AlexNet</a>, <a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/cnn/vgg19.py">VGG19</a> or <a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/cnn/resnet50.py">ResNet</a>.</p>
<p>For unique problems that don’t have pre-trained networks the classic and simple hand-tuning is a great way to start. A few iterations can give you a good architecture which won’t be the state-of-the-art but should give you satisfying result with a minimum of problems. In case if accuracy does not suffice your needs you can always boost your performance getting more data or developing ensembles with different models.</p>
</div>
<div class="section" id="source-code">
<h2><a class="toc-backref" href="#id29">Source Code</a></h2>
<p>All source code is available on GitHub in the <a class="reference external" href="https://github.com/itdxer/neupy/blob/master/notebooks/Hyperparameter%20optimization%20for%20Neural%20Networks.ipynb">iPython notebook</a>. It includes all visualizations and hyperparameter selection algorithms.</p>
</div>
<div class="section" id="references">
<h2><a class="toc-backref" href="#id30">References</a></h2>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[1]</a></td><td>Bayesian Optimization and Acquisition Functions from <a class="reference external" href="http://www.cse.wustl.edu/~garnett/cse515t/files/lecture_notes/12.pdf">http://www.cse.wustl.edu/~garnett/cse515t/files/lecture_notes/12.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id6" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Gaussian Processes in Machine Learning from <a class="reference external" href="http://mlg.eng.cam.ac.uk/pub/pdf/Ras04.pdf">http://mlg.eng.cam.ac.uk/pub/pdf/Ras04.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id7" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[3]</a></td><td>Slides: Introduction to Gaussian Process from <a class="reference external" href="https://www.cs.toronto.edu/~hinton/csc2515/notes/gp_slides_fall08.pdf">https://www.cs.toronto.edu/~hinton/csc2515/notes/gp_slides_fall08.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id8" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[4]</a></td><td>Preliminary Evaluation of Hyperopt Algorithms on HPOLib from <a class="reference external" href="http://compneuro.uwaterloo.ca/files/publications/bergstra.2014.pdf">http://compneuro.uwaterloo.ca/files/publications/bergstra.2014.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id9" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label">[5]</td><td>Algorithms for Hyper-Parameter Optimization from <a class="reference external" href="http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf">http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id10" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label">[6]</td><td>Slides: Pattern Recognition, Lecture 6 from <a class="reference external" href="http://www.csd.uwo.ca/~olga/Courses/CS434a_541a/Lecture6.pdf">http://www.csd.uwo.ca/~olga/Courses/CS434a_541a/Lecture6.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id11" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[7]</a></td><td>Low-discrepancy sampling methods from <a class="reference external" href="http://planning.cs.uiuc.edu/node210.html">http://planning.cs.uiuc.edu/node210.html</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id12" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label">[8]</td><td>Parzen-Window Density Estimation from <a class="reference external" href="https://www.cs.utah.edu/~suyash/Dissertation_html/node11.html">https://www.cs.utah.edu/~suyash/Dissertation_html/node11.html</a></td></tr>
</tbody>
</table>
</div>

        </div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Yurii Shevchuk</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="tags/visualization.html">visualization</a>, <a href="tags/backpropagation.html">backpropagation</a>, <a href="tags/supervised.html">supervised</a>, <a href="tags/hyperparameter_optimization.html">hyperparameter optimization</a></span>
        </div>
        <div class="comments">
            <a href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#disqus_thread" data-disqus-identifier="2016/12/17/hyperparameter_optimization_for_neural_networks">Leave a comment</a>
        </div></div><div class="separator post_separator"></div><div class="timestamp postmeta">
            <span>November 12, 2016</span>
        </div>
        <div class="section">
            <span id="id1"/><h1><a href="2016/11/12/mnist_classification.html">MNIST Classification</a></h1>
<p>The MNIST problem is probably the most known for those who have already
heared about neural networks. This short tutorial shows you how to build prediction models in NeuPy. Let’s start developing model.</p>
<p>First of all we need to load data.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">model_selection</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mnist</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_mldata</span><span class="p">(</span><span class="s1">'MNIST original'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">mnist</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
<p>I used scikit-learn to fetch the MNIST dataset, but you can load it in different way.</p>
<p>Data doesn’t have appropriate format for neural network, so we need to make simple transformation before use it.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span> <span class="o">/</span> <span class="mf">255.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span> <span class="o">-</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_scaler</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">target_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">todense</span><span class="p">()</span>
</pre></div>
</div>
<p>Next we need to divide dataset into two parts: train and test. Regarding <a class="reference external" href="http://yann.lecun.com/exdb/mnist/">The
MNIST Database</a> page we will use 60,000
samples for training and 10,000 for test.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">environment</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">environment</span><span class="o">.</span><span class="n">reproducible</span><span class="p">()</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">target</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">train_size</span><span class="o">=</span><span class="p">(</span><span class="mf">6.</span> <span class="o">/</span> <span class="mi">7</span><span class="p">)</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>In the previous procedure I converted all data to <cite>float32</cite> data type. This
simple trick will help us use less memory and decrease computational time.
Theano is a main backend for the Gradient Descent based algorithms in NeuPy.
For the Theano we need to add additional configuration that will explain Theano that
we are going to use 32bit float numbers.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">theano</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span> <span class="o">=</span> <span class="s1">'float32'</span>
</pre></div>
</div>
<p>We prepared everything that we need for the neural network training. Now we are
able to create the neural network that will classify digits for us.</p>
<p>Let’s start with an architecture. I didn’t reinvent the wheel and used one of the know architectures from <a class="reference external" href="http://yann.lecun.com/exdb/mnist/">The Database</a> page which is 784 &gt; 500 &gt; 300 &gt; 10. As the main activation function I used Relu and Softmax for the final layer. The main algorithm is a Nesterov Momentum that uses 100 samples per batch iteration. Actually all this and other network configuration should be clear from the code shown below.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span><span class="p">,</span> <span class="n">layers</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span>
<span class="gp">... </span>        <span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="mi">784</span><span class="p">),</span>
<span class="gp">... </span>        <span class="n">layers</span><span class="o">.</span><span class="n">Relu</span><span class="p">(</span><span class="mi">500</span><span class="p">),</span>
<span class="gp">... </span>        <span class="n">layers</span><span class="o">.</span><span class="n">Relu</span><span class="p">(</span><span class="mi">300</span><span class="p">),</span>
<span class="gp">... </span>        <span class="n">layers</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
<span class="gp">... </span>    <span class="p">],</span>
<span class="gp">... </span>    <span class="n">error</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">step</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">shuffle_data</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>Isn’t it simple and clear? All the most important information related to the neural network you can find in the terminal output. If you run the code that shown above you would get the same output as on the figure below.</p>
<a class="reference internal image-reference" href="_images/bpnet-config-logs.png"><img alt="Gradient Descent configuration" class="align-center" src="_images/bpnet-config-logs.png" style="width: 70%;"/></a>
<p>From this output we can extract a lot of information about network configurations.</p>
<p>First of all, as we can see, most of options have green color label, but some of them are gray. Green color defines all options which we put in network manually and gray color options are default parameters. All properties separeted on few groups and each group is a <a class="reference internal" href="apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a>  parent classes. More information about <a class="reference internal" href="apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> algorithm properties you will find in documentation, just click on algorithm name link and you will see it.</p>
<p>In addition for feedforward neural networks it’s possible to check architecture in form of a table.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">network</span><span class="o">.</span><span class="n">architecture</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="_images/bpnet-architecture.png"><img alt="Neural Network Architecture" class="align-center" src="_images/bpnet-architecture.png" style="width: 70%;"/></a>
<p>Now we are going to train network. Let set up 20 epochs for training procedure and check the result.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">network</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
<p>Output in terminal should look similar to this one:</p>
<a class="reference internal image-reference" href="_images/bpnet-train-logs.png"><img alt="GradientDescent training procedure output" class="align-center" src="_images/bpnet-train-logs.png" style="width: 70%;"/></a>
<p>Output show the most important information related to training procedure. Each epoch contains 4 columns. First one identified epoch. The second one show training error. The third one is optional. In case you have validation dataset, you can check learning perfomanse using dataset separated from the learning procedure. And the last column shows how many time network trains during this epoch.</p>
<p>From the table is not clear network’s training progress. We can check it very easy. Network instance contains built-in method that build line plot that show training progress. Let’s check our progress.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">plots</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plots</span><span class="o">.</span><span class="n">error_plot</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="_images/bpnet-train-errors-plot.png"><img alt="GradientDescent epoch errors plot" class="align-center" src="_images/bpnet-train-errors-plot.png" style="width: 70%;"/></a>
<p>From the figure above you can notice that validation error does not decrease over time. Sometimes it goes up and sometimes down, but it doesn’t mean that network trains poorly. Let’s check small example that can make this problem clear.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">actual_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model1_prediction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model2_prediction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">])</span>
</pre></div>
</div>
<p>In the code above you can see two prediction releate to the different models. The first model predicted two samples right and one wrong. The second one predicted everything right. But second model’s predictions are less certain. Let’s check the cross entropy error.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">estimators</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimators</span><span class="o">.</span><span class="n">binary_crossentropy</span><span class="p">(</span><span class="n">actual_values</span><span class="p">,</span> <span class="n">model1_prediction</span><span class="p">)</span>
<span class="go">0.3756706118583679</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimators</span><span class="o">.</span><span class="n">binary_crossentropy</span><span class="p">(</span><span class="n">actual_values</span><span class="p">,</span> <span class="n">model2_prediction</span><span class="p">)</span>
<span class="go">0.5108255743980408</span>
</pre></div>
</div>
<p>That is the result that we looked for. The second model made better prediction, but it got a higher cross entropy error. It means that we less certain about our prediction. Similar situation we’ve observed in the plot above.</p>
<p>Let’s finally make a simple report for our classification result.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_predicted</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">))</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">))</span>
<span class="go">        precision    recall  f1-score   support</span>

<span class="go">    0       0.98      0.99      0.99       936</span>
<span class="go">    1       0.99      0.99      0.99      1163</span>
<span class="go">    2       0.98      0.98      0.98       982</span>
<span class="go">    3       0.98      0.99      0.98      1038</span>
<span class="go">    4       0.98      0.98      0.98       948</span>
<span class="go">    5       0.99      0.98      0.98       921</span>
<span class="go">    6       0.99      0.99      0.99      1013</span>
<span class="go">    7       0.98      0.98      0.98      1029</span>
<span class="go">    8       0.98      0.98      0.98       978</span>
<span class="go">    9       0.98      0.96      0.97       992</span>

<span class="go">    avg / total       0.98      0.98      0.98     10000</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="s2">"Validation accuracy: {:.2%}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score</span><span class="p">))</span>
<span class="go">Validation accuracy: 98.37%</span>
</pre></div>
</div>
<p>The 98.37% accuracy is pretty good for such a quick solution. Additional modification can improve prediction accuracy.</p>

        </div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Yurii Shevchuk</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="tags/classification.html">classification</a>, <a href="tags/tutorials.html">tutorials</a>, <a href="tags/supervised.html">supervised</a>, <a href="tags/backpropagation.html">backpropagation</a>, <a href="tags/image_recognition.html">image recognition</a>, <a href="tags/deep_learning.html">deep learning</a></span>
        </div>
        <div class="comments">
            <a href="http://neupy.com/2016/11/12/mnist_classification.html#disqus_thread" data-disqus-identifier="2016/11/12/mnist_classification">Leave a comment</a>
        </div></div><div class="separator post_separator"></div><div class="timestamp postmeta">
            <span>September 21, 2015</span>
        </div>
        <div class="section">
            <span id="id1"/><h1><a href="2015/09/21/password_recovery.html"><a class="toc-backref" href="#id2">Password recovery</a></a></h1>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="2015/09/21/password_recovery.html#password-recovery" id="id2">Password recovery</a><ul>
<li><a class="reference internal" href="2015/09/21/password_recovery.html#data-transformation" id="id3">Data transformation</a></li>
<li><a class="reference internal" href="2015/09/21/password_recovery.html#saving-password-into-the-network" id="id4">Saving password into the network</a></li>
<li><a class="reference internal" href="2015/09/21/password_recovery.html#recovering-password-from-the-network" id="id5">Recovering password from the network</a></li>
<li><a class="reference internal" href="2015/09/21/password_recovery.html#test-it-using-monte-carlo" id="id6">Test it using Monte Carlo</a></li>
<li><a class="reference internal" href="2015/09/21/password_recovery.html#possible-problems" id="id7">Possible problems</a></li>
<li><a class="reference internal" href="2015/09/21/password_recovery.html#summary" id="id8">Summary</a></li>
<li><a class="reference internal" href="2015/09/21/password_recovery.html#download-script" id="id9">Download script</a></li>
</ul>
</li>
</ul>
</div>
<p>In this article we are going to build a simple neural network that will recover password from a broken one.
If you aren’t familiar with a <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> algorithm, you can read <a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#discrete-hopfield-network"><span>this article</span></a>.</p>
<p>Before running all experiments, we need to set up <span class="docutils literal"><span class="pre">seed</span></span> parameter to make all results reproducible.
But you can test code without it.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">environment</span>
<span class="n">environment</span><span class="o">.</span><span class="n">reproducible</span><span class="p">()</span>
</pre></div>
</div>
<p>If you can’t reproduce with your version of Python or libraries you can install those ones that were used in this article:</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">neupy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neupy</span><span class="o">.</span><span class="n">__version__</span>
<span class="go">'0.3.0'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">numpy</span><span class="o">.</span><span class="n">__version__</span>
<span class="go">'1.9.2'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">platform</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">platform</span><span class="o">.</span><span class="n">python_version</span><span class="p">()</span>
<span class="go">'3.4.3'</span>
</pre></div>
</div>
<p>Code works with a Python 2.7 as well.</p>
<div class="section" id="data-transformation">
<h2><a class="toc-backref" href="#id3">Data transformation</a></h2>
<p>Before building the network that will save and recover passwords, we should make transformations for input and output data.
But it wouldn’t be enough just to encode it, we should set up a constant length for an input string to make sure that strings will have the same length
Also we should define what string encoding we will use.
For simplicity we will use only ASCII symbols.
So, let’s define a function that transforms a string into a binary list.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">str2bin</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Text can't contains more "</span>
                         <span class="s2">"than {} symbols"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">max_length</span><span class="p">))</span>

    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">rjust</span><span class="p">(</span><span class="n">max_length</span><span class="p">)</span>

    <span class="n">bits_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
        <span class="n">bits</span> <span class="o">=</span> <span class="nb">bin</span><span class="p">(</span><span class="nb">ord</span><span class="p">(</span><span class="n">symbol</span><span class="p">))</span>
        <span class="c1"># Cut `0b` from the beggining and fill with zeros if they</span>
        <span class="c1"># are missed</span>
        <span class="n">bits</span> <span class="o">=</span> <span class="n">bits</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span><span class="o">.</span><span class="n">zfill</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
        <span class="n">bits_list</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">bits</span><span class="p">))</span>

    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">bits_list</span><span class="p">)</span>
</pre></div>
</div>
<p>Our function takes 2 parameters.
First one is the string that we want to encode.
And second attribute is setting up a constant length for input vector.
If length of the input string is less than <span class="docutils literal"><span class="pre">max_length</span></span> value, then function fills spaces at the beginning of the string.</p>
<p>Let’s check <span class="docutils literal"><span class="pre">str2bin</span></span> function output.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">str2bin</span><span class="p">(</span><span class="s2">"test"</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="go">[0, 0, 1, 0, 0, 0, 0, 0, 0, ... ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">str2bin</span><span class="p">(</span><span class="s2">"test"</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
<span class="go">40</span>
</pre></div>
</div>
<p>ASCII encoding uses 8 bits per symbol and we set up 5 symbols per string, so our vector length equals to 40.
From the first output, as you can see, first 8 symbols are equal to <span class="docutils literal"><span class="pre">00100000</span></span>, that is a space value from the ASCII table.</p>
<p>After preforming recovery procedure we will always be getting a binary list.
So before we begin to store data in neural network, we should define another function that transforms a binary list back into a string (which is basically inversed operation to the previous function).</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">chunker</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">position</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">),</span> <span class="n">size</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">sequence</span><span class="p">[</span><span class="n">position</span><span class="p">:</span><span class="n">position</span> <span class="o">+</span> <span class="n">size</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">bin2str</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="n">characters</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">binary_symbol_code</span> <span class="ow">in</span> <span class="n">chunker</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="n">binary_symbol_str</span> <span class="o">=</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">binary_symbol_code</span><span class="p">))</span>
        <span class="n">character</span> <span class="o">=</span> <span class="nb">chr</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">binary_symbol_str</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
        <span class="n">characters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">character</span><span class="p">)</span>
    <span class="k">return</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">characters</span><span class="p">)</span><span class="o">.</span><span class="n">lstrip</span><span class="p">()</span>
</pre></div>
</div>
<p>If we test this function we will get word <span class="docutils literal"><span class="pre">test</span></span> back.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">bin2str</span><span class="p">(</span><span class="n">str2bin</span><span class="p">(</span><span class="s2">"test"</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
<span class="go">'test'</span>
</pre></div>
</div>
<p>Pay attention! Function has removed all spaces at the beggining of the string before bringing them back.
We assume that password won’t contain space at the beggining.</p>
</div>
<div class="section" id="saving-password-into-the-network">
<h2><a class="toc-backref" href="#id4">Saving password into the network</a></h2>
<p>Now we are ready to save the password into the network.
For this task we are going to define another function that create network and save password inside of it.
Let’s define this function and later we will look at it step by step.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span>

<span class="k">def</span> <span class="nf">save_password</span><span class="p">(</span><span class="n">real_password</span><span class="p">,</span> <span class="n">noise_level</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">noise_level</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"`noise_level` must be equal or greater than 1."</span><span class="p">)</span>

    <span class="n">binary_password</span> <span class="o">=</span> <span class="n">str2bin</span><span class="p">(</span><span class="n">real_password</span><span class="p">)</span>
    <span class="n">bin_password_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">binary_password</span><span class="p">)</span>

    <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">binary_password</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">noise_level</span><span class="p">):</span>
        <span class="c1"># The farther from the 0.5 value the less likely</span>
        <span class="c1"># password recovery</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="n">bin_password_len</span><span class="p">)</span>
        <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>

    <span class="n">dhnet</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">DiscreteHopfieldNetwork</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">'sync'</span><span class="p">)</span>
    <span class="n">dhnet</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">dhnet</span>
</pre></div>
</div>
<p>If you have already read <a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#discrete-hopfield-network"><span>Discrete Hopfield Network article</span></a>, you should know that if we add only one vector into the network we will get it dublicated or with reversed signs through the whole matrix.
To make it a little bit secure we can add some noise into the network.
For this reason we introduce one additional parameter <span class="docutils literal"><span class="pre">noise_level</span></span> into the function.
This parameter controls number of randomly generated binary vectors.
With each iteration using Binomial distribution we generate random binary vector with 55% probability of getting 1 in <cite>noise</cite> vector.
And then we put all the noise vectors and transformed password into one matrix.
And finaly we save all data in the <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.</p>
<p>And that’s it.
Function returns trained network for a later usage.</p>
<p>But why do we use random binary vectors instead of the decoded random strings?
The problem is in the similarity between two vectors.
Let’s check two approaches and compare them with a <a class="reference external" href="https://en.wikipedia.org/wiki/Hamming_distance">Hamming distance</a>.
But before starting we should define a function that measures distance between two vectors.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="k">def</span> <span class="nf">hamming_distance</span><span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">):</span>
    <span class="n">left</span><span class="p">,</span> <span class="n">right</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">left</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">right</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">left</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">right</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Shapes must be equal"</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">left</span> <span class="o">!=</span> <span class="n">right</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">generate_password</span><span class="p">(</span><span class="n">min_length</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">symbols</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
        <span class="n">string</span><span class="o">.</span><span class="n">ascii_letters</span> <span class="o">+</span>
        <span class="n">string</span><span class="o">.</span><span class="n">digits</span> <span class="o">+</span>
        <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span>
    <span class="p">)</span>
    <span class="n">password_len</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randrange</span><span class="p">(</span><span class="n">min_length</span><span class="p">,</span> <span class="n">max_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">password</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">password_len</span><span class="p">)]</span>
    <span class="k">return</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">password</span><span class="p">)</span>
</pre></div>
</div>
<p>In addition you can see the <span class="docutils literal"><span class="pre">generate_password</span></span> function that we will use for tests.
Let’s check Hamming distance between two randomly generate password vectors.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">hamming_distance</span><span class="p">(</span><span class="n">str2bin</span><span class="p">(</span><span class="n">generate_password</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span>
<span class="gp">... </span>                 <span class="n">str2bin</span><span class="p">(</span><span class="n">generate_password</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)))</span>
<span class="go">70</span>
</pre></div>
</div>
<p>As we can see two randomly generated passwords are very similar to each other (approximetly 70% (<span class="math">\(100 * (240 - 70) / 240\)</span>) of bits are the same).
But If we compare randomly generated password to random binary vector we will see the difference.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">hamming_distance</span><span class="p">(</span><span class="n">str2bin</span><span class="p">(</span><span class="n">generate_password</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span>
<span class="gp">... </span>                 <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mi">240</span><span class="p">))</span>
<span class="go">134</span>
</pre></div>
</div>
<p>Hamming distance is bigger than in the previous example.
A little bit more than 55% of the bits are different.</p>
<p>The greater the difference between them the easier recovery procedure for the input vectors patterns from the network.
For this reason we use randomly generated binary vector instead of random password.</p>
<p>Of course it’s better to save not randomly generated noise vectors but randomly generated passwords converted into binary vectors, cuz if you use wrong input pattern randomly generated password might be recovered instead of the correct one.</p>
</div>
<div class="section" id="recovering-password-from-the-network">
<h2><a class="toc-backref" href="#id5">Recovering password from the network</a></h2>
<p>Now we are going to define the last function which will recover a password from the network.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">recover_password</span><span class="p">(</span><span class="n">dhnet</span><span class="p">,</span> <span class="n">broken_password</span><span class="p">):</span>
    <span class="n">test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">str2bin</span><span class="p">(</span><span class="n">broken_password</span><span class="p">))</span>
    <span class="n">recovered_password</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">recovered_password</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">recovered_password</span> <span class="o">=</span> <span class="n">recovered_password</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>

    <span class="k">return</span> <span class="n">bin2str</span><span class="p">(</span><span class="n">recovered_password</span><span class="p">)</span>
</pre></div>
</div>
<p>Function takes two parameters.
The first one is network example from which function will recover a password from a broken one.
And the second parameter is a broken password.</p>
<p>Finnaly we can test password recovery from the network.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">my_password</span> <span class="o">=</span> <span class="s2">"$My%Super^Secret*^&amp;Passwd"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span> <span class="o">=</span> <span class="n">save_password</span><span class="p">(</span><span class="n">my_password</span><span class="p">,</span> <span class="n">noise_level</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recover_password</span><span class="p">(</span><span class="n">dhnet</span><span class="p">,</span> <span class="s2">"-My-Super-Secret---Passwd"</span><span class="p">)</span>
<span class="go">'$My%Super^Secret*^&amp;Passwd'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">==</span> <span class="n">my_password</span>
<span class="go">True</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recover_password</span><span class="p">(</span><span class="n">dhnet</span><span class="p">,</span> <span class="s2">"-My-Super"</span><span class="p">)</span>
<span class="go">'\x19`\xa0\x04Í\x14#ÛE2er\x1eÛe#2m4jV\x07PqsCwd'</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recover_password</span><span class="p">(</span><span class="n">dhnet</span><span class="p">,</span> <span class="s2">"Invalid"</span><span class="p">)</span>
<span class="go">'\x02 \x1d`\x80$Ì\x1c#ÎE¢eò\x0eÛe§:/$ê\x04\x07@5sCu$'</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recover_password</span><span class="p">(</span><span class="n">dhnet</span><span class="p">,</span> <span class="s2">"MySuperSecretPasswd"</span><span class="p">)</span>
<span class="go">'$My%Super^Secret*^&amp;Passwd'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">==</span> <span class="n">my_password</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Everithing looks fine.
After multiple times code running you can rarely find a problem.
Network can produce a string which wasn’t taught.
This string can look almost like a password with a few different symbols.
The problem appears when network creates additional local minimum somewhere between input patterns.
We can’t prevent it from running into the local minimum.
For more information about this problem you can check <a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#discrete-hopfield-network"><span>article about Discrete Hopfield Network</span></a>.</p>
</div>
<div class="section" id="test-it-using-monte-carlo">
<h2><a class="toc-backref" href="#id6">Test it using Monte Carlo</a></h2>
<p>Let’s test our solution with randomly generated passwords.
For this task we can use Monte Carlo experiment.
At each step we create random password and try to recover it from a broken password.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">pprint</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>

<span class="k">def</span> <span class="nf">cutword</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">fromleft</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">fromleft</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="o">-</span><span class="n">k</span><span class="p">:]</span> <span class="k">if</span> <span class="n">k</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="s1">''</span><span class="p">)</span><span class="o">.</span><span class="n">rjust</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">word</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span> <span class="k">if</span> <span class="n">k</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="s1">''</span><span class="p">)</span><span class="o">.</span><span class="n">ljust</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">))</span>

<span class="n">n_times</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">cases</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">'exclude-one'</span><span class="p">,</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">'exclude-quarter'</span><span class="p">,</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">x</span> <span class="o">//</span> <span class="mi">4</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">'exclude-half'</span><span class="p">,</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">'just-one-symbol'</span><span class="p">,</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">'empty-string'</span><span class="p">,</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">0</span><span class="p">)),</span>
<span class="p">])</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="n">cases</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_times</span><span class="p">):</span>
    <span class="n">real_password</span> <span class="o">=</span> <span class="n">generate_password</span><span class="p">(</span><span class="n">min_length</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">casename</span><span class="p">,</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">cases</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">n_letters</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">real_password</span><span class="p">))</span>
        <span class="n">broken_password</span> <span class="o">=</span> <span class="n">cutword</span><span class="p">(</span><span class="n">real_password</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">n_letters</span><span class="p">,</span>
                                  <span class="n">fromleft</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">dhnet</span> <span class="o">=</span> <span class="n">save_password</span><span class="p">(</span><span class="n">real_password</span><span class="p">,</span> <span class="n">noise_level</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
        <span class="n">recovered_password</span> <span class="o">=</span> <span class="n">recover_password</span><span class="p">(</span><span class="n">dhnet</span><span class="p">,</span> <span class="n">broken_password</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">recovered_password</span> <span class="o">!=</span> <span class="n">real_password</span><span class="p">:</span>
            <span class="n">results</span><span class="p">[</span><span class="n">casename</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="k">print</span><span class="p">(</span><span class="s2">"Number of fails for each test case:"</span><span class="p">)</span>
<span class="n">pprint</span><span class="o">.</span><span class="n">pprint</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
<p>After sumbmission your output should look the same as the one below (if you followed everything step by step):</p>
<div class="highlight-python"><div class="highlight"><pre><span/>Number of fails for each test case:
{'exclude-one': 11,
 'exclude-quarter': 729,
 'exclude-half': 5823,
 'just-one-symbol': 9998,
 'empty-string': 10000}
</pre></div>
</div>
<p>At this test we catch two situations when the network recovers the password from one symbol, which is not very good.
It really depends on the noise which we stored inside the network.
Randomization can’t give you perfect results.
Sometimes it can recover a password from an empty string, but such situation is also very rare.</p>
<p>In the last test, on each iteration we cut password from the left side and filled other parts with spaces.
Let’s test another approach.
Let’s cut a password from the right side and see what we’ll get:</p>
<div class="highlight-python"><div class="highlight"><pre><span/>Number of fails for each test case:
{'exclude-one': 17,
 'exclude-quarter': 705,
 'exclude-half': 5815,
 'just-one-symbol': 9995,
 'empty-string': 10000}
</pre></div>
</div>
<p>Results look similar to the previous test.</p>
<p>Another interesting test can take place if you randomly replace some symbols with spaces:</p>
<div class="highlight-python"><div class="highlight"><pre><span/>Number of fails for each test case:
{'exclude-one': 14,
 'exclude-quarter': 749,
 'exclude-half': 5760,
 'just-one-symbol': 9998,
 'empty-string': 10000}
</pre></div>
</div>
<p>The result is very similar to the previous two.</p>
<p>And finally, instead of replacing symbols with spaces we can remove symbols without any replacements.
Results do not look good:</p>
<div class="highlight-python"><div class="highlight"><pre><span/>Number of fails for each test case:
{'exclude-one': 3897,
 'exclude-quarter': 9464,
 'exclude-half': 9943,
 'just-one-symbol': 9998,
 'empty-string': 9998}
</pre></div>
</div>
<p>I guess in first case (<span class="docutils literal"><span class="pre">exclude-one</span></span>) we just got lucky and after eliminating one symbol from the end didn’t shift most of the symbols.
So removing symbols is not a very good idea.</p>
<p>All functions that you need for experiments you can find at the <a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/memory/password_recovery.py">github</a>.</p>
</div>
<div class="section" id="possible-problems">
<h2><a class="toc-backref" href="#id7">Possible problems</a></h2>
<p>There are a few possible problems in the Discrete Hopfile Network.</p>
<ol class="arabic simple">
<li>As we saw from the last experiments, shifted passwords are harder to recover than the passwords with missed symbols. It’s better to replace missed symbols with some other things.</li>
<li>There already exists small probability for recovering passwords from empty strings.</li>
<li>Similar binary code representation for different symbols is a big problem. Sometimes you can have a situation where two symbols in binary code represantation are different just by one bit. The first solution is to use a One Hot Encoder. But it can give us even more problems. For example, we used symbols from list of 94 symbols for the password. If we encode each symbol we will get a vector with 93 zeros and just one active value. The problem is that after the recovery procedure we should always get 1 active value, but this situation is very unlikely to happen.</li>
</ol>
</div>
<div class="section" id="summary">
<h2><a class="toc-backref" href="#id8">Summary</a></h2>
<p>Despite some problems, network recovers passwords very well.
Monte Carlo experiment shows that the fewer symbols we know the less is probability for recovering them correctly.</p>
<p>Even this simple network can be a powerful tool if you know its limitations.</p>
</div>
<div class="section" id="download-script">
<h2><a class="toc-backref" href="#id9">Download script</a></h2>
<p>You can download and test a full script from the <a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/memory/password_recovery.py">github repository</a>.</p>
<p>It doesn’t contain a fixed <span class="docutils literal"><span class="pre">environment.reproducible</span></span> function, so you will get different outputs after each run.</p>
</div>

        </div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Yurii Shevchuk</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="tags/memory.html">memory</a>, <a href="tags/unsupervised.html">unsupervised</a></span>
        </div>
        <div class="comments">
            <a href="http://neupy.com/2015/09/21/password_recovery.html#disqus_thread" data-disqus-identifier="2015/09/21/password_recovery">Leave a comment</a>
        </div></div><div class="separator post_separator"></div><div class="timestamp postmeta">
            <span>September 20, 2015</span>
        </div>
        <div class="section">
            <span id="id1"/><h1><a href="2015/09/20/discrete_hopfield_network.html"><a class="toc-backref" href="#id6">Discrete Hopfield Network</a></a></h1>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#discrete-hopfield-network" id="id6">Discrete Hopfield Network</a><ul>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#architecture" id="id7">Architecture</a><ul>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#training-procedure" id="id8">Training procedure</a></li>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#recovery-from-memory" id="id9">Recovery from memory</a><ul>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#synchronous" id="id10">Synchronous</a></li>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#asynchronous" id="id11">Asynchronous</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#memory-limit" id="id12">Memory limit</a></li>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#hallucinations" id="id13">Hallucinations</a></li>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#example" id="id14">Example</a></li>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#more-reading" id="id15">More reading</a></li>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#references" id="id16">References</a></li>
</ul>
</li>
</ul>
</div>
<p>In this article we are going to learn about <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> algorithm.</p>
<p><a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> is a type of algorithms which is called - <a class="reference external" href="https://en.wikipedia.org/wiki/Autoassociative_memory">Autoassociative memories</a>
Don’t be scared of the word <cite>Autoassociative</cite>.
The idea behind this type of algorithms is very simple.
It can store useful information in <cite>memory</cite> and later it is able to reproduce this information from partialy broken patterns.
You can preceive it as human memory.
For instance, imagine that you look at an old picture of a place where you were long time ago, but this picture is of very bad quality and very blurry.
By looking at the picture you manage to recognize a few objects or places that make sense to you and form some objects even though they are blurry.
It can be a house, a lake or anything that can add up to the whole picture and bring out some associations about this place.
With these details that you got from your memory so far other parts of picture start to make even more sense.
Though you don’t clearly see all objects in the picture, you start to remember things and withdraw from your memory some images, that cannot be seen in the picture, just because of those very familiarly-shaped details that you’ve got so far.
That’s what it is all about.
Autoassociative memory networks is a posibily to interprete functions of memory into neural network model.</p>
<p>Don’t worry if you have only basic knowledge in Linear Algebra; in this article I’ll try to explain the idea as simple as possible.
If you are interested in proofs of the <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> you can check them at R. Rojas. Neural Networks <a class="footnote-reference" href="#id3" id="id2">[1]</a> book.</p>
<div class="section" id="architecture">
<h2><a class="toc-backref" href="#id7">Architecture</a></h2>
<p><a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> is an easy algorithm.
It’s simple because you don’t need a lot of background knowledge in Maths for using it.
Everything you need to know is how to make a basic Linear Algebra operations, like outer product or sum of two matrices.</p>
<p>Let’s begin with a basic thing.
What do we know about this neural network so far?
Just the name and the type.
From the name we can identify one useful thing about the network.
It’s <cite>Discrete</cite>.
It means that network only works with binary vectors.
But for this network we wouldn’t use binary numbers in a typical form.
Instead, we will use bipolar numbers.
They are almost the same, but instead of 0 we are going to use -1 to decode a negative state.
We can’t use zeros.
And there are two main reasons for it.
The first one is that zeros reduce information from the network weight, later in this article you are going to see it.
The second one is more complex, it depends on the nature of bipolar vectors.
Basically they are more likely to be orthogonal to each other which is a critical moment for the <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.
But as I mentioned before we won’t talk about proofs or anything not related to basic understanding of Linear Algebra operations.</p>
<p>So, let’s look at how we can train and use the <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.</p>
<div class="section" id="training-procedure">
<h3><a class="toc-backref" href="#id8">Training procedure</a></h3>
<p>We can’t use memory without any patterns stored in it.
So first of all we are going to learn how to train the network.
For the <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> train procedure doesn’t require any iterations.
It includes just an outer product between input vector and transposed input vector.</p>
<div class="math">
\[\begin{split}\begin{align*}
    W = x \cdot x^T =
    \left[
    \begin{array}{c}
      x_1\\
      x_2\\
      \vdots\\
      x_n
    \end{array}
    \right]
    \cdot
    \left[
    \begin{array}{c}
      x_1 &amp; x_2 &amp; \cdots &amp; x_n
    \end{array}
    \right]
\end{align*}
=\end{split}\]</div>
<div class="math">
\[\begin{split}\begin{align*}
    =
    \left[
    \begin{array}{c}
      x_1^2 &amp; x_1 x_2 &amp; \cdots &amp; x_1 x_n \\
      x_2 x_1 &amp; x_2^2 &amp; \cdots &amp; x_2 x_n \\
      \vdots\\
      x_n x_1 &amp; x_n x_2 &amp; \cdots &amp; x_n^2 \\
    \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p><span class="math">\(W\)</span> is a weight matrix and <span class="math">\(x\)</span> is an input vector.
Each value <span class="math">\(x_i\)</span> in the input vector can only be -1 or 1.
So on the matrix diagonal we only have squared values and it means we will always see 1s at those places.
Think about it, everytime, in any case, values on the diagonal can take just one possible state.
We can’t use this information, because it doesn’t say anything useful about patterns that are stored in the memory and even can make incorrect contribution into the output result.
For this reason we need to set up all the diagonal values equal to zero.
The final weight formula should look like this one below.</p>
<div class="math">
\[\begin{split}\begin{align*}
    W =
    x x^T - I =
    \left[
    \begin{array}{c}
      0 &amp; x_1 x_2 &amp; \cdots &amp; x_1 x_n \\
      x_2 x_1 &amp; 0 &amp; \cdots &amp; x_2 x_n \\
      \vdots\\
      x_n x_1 &amp; x_n x_2 &amp; \cdots &amp; 0 \\
    \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>Where <span class="math">\(I\)</span> is an identity matrix.</p>
<p>But usualy we need to store more values in memory.
For another pattern we have to do exacly the same procedure as before and then just add the generated weight matrix to the old one.</p>
<div class="math">
\[W = W_{old} + W_{new}\]</div>
<p>And this procedure generates us a new weight that would be valid for both previously stored patterns.
Later you can add other patterns using the same algorithm.</p>
<p>But if you need to store multiple vectors inside the network at the same time you don’t need to compute the weight for each vector and then sum them up.
If you have a matrix <span class="math">\(X \in \Bbb R^{m\times n}\)</span> where each row is the input vector, then you can just make product matrix between transposed input matrix and input matrix.</p>
<div class="math">
\[W = X^T X - m I\]</div>
<p>Where <span class="math">\(I\)</span> is an identity matrix (<span class="math">\(I \in \Bbb R^{n\times n}\)</span>), <span class="math">\(n\)</span> is a number of features in the input vector and <span class="math">\(m\)</span> is a number of input patterns inside the matrix <span class="math">\(X\)</span>.
Term <span class="math">\(m I\)</span> removes all values from the diagonal.
Basically we remove 1s for each stored pattern and since we have <span class="math">\(m\)</span> of them, we should do it <span class="math">\(m\)</span> times.
Practically, it’s not very good to create an identity matrix just to set up zeros on the diagonal, especially when dimension on the matrix is very big.
Usually linear algebra libraries give you a possibility to set up diagonal values without creating an additional matrix and this solution would be more efficient.
For example in NumPy library it’s a <a class="reference external" href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.fill_diagonal.html">numpy.fill_diagonal</a> function.</p>
<p>Let’s check an example just to make sure that everything is clear.
Let’s pretend we have a vector <span class="math">\(u\)</span>.</p>
<div class="math">
\[\begin{split}u = \left[\begin{align*}1 \\ -1 \\ 1 \\ -1\end{align*}\right]\end{split}\]</div>
<p>Assume that network doesn’t have patterns inside of it, so the vector <span class="math">\(u\)</span> would be the first one.
Let’s compute weights for the network.</p>
<div class="math">
\[\begin{split}\begin{align*}
    U = u u^T =
    \left[
        \begin{array}{c}
            1 \\
            -1 \\
            1 \\
            -1
        \end{array}
    \right]
    \left[
        \begin{array}{c}
            1 &amp; -1 &amp; 1 &amp; -1
        \end{array}
    \right]
    =
    \left[
        \begin{array}{cccc}
            1 &amp; -1 &amp; 1 &amp; -1\\
            -1 &amp; 1 &amp; -1 &amp; 1\\
            1 &amp; -1 &amp; 1 &amp; -1\\
            -1 &amp; 1 &amp; -1 &amp; 1
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>Look closer to the matrix <span class="math">\(U\)</span> that we got.
Outer product just repeats vector 4 times with the same or inversed values.
First and third columns (or rows, it doesn’t matter, because matrix is symmetrical) are exacly the same as the input vector.
The second and fourth are also the same, but with an opposite sign.
That’s because in the vector <span class="math">\(u\)</span> we have 1 on the first and third places and -1 on the other.</p>
<p>To make weight from the <span class="math">\(U\)</span> matrix, we need to remove ones from the diagonal.</p>
<div class="math">
\[\begin{split}W = U - I = \left[
    \begin{array}{cccc}
        1 &amp; -1 &amp; 1 &amp; -1\\
        -1 &amp; 1 &amp; -1 &amp; 1\\
        1 &amp; -1 &amp; 1 &amp; -1\\
        -1 &amp; 1 &amp; -1 &amp; 1
    \end{array}
\right] -
\left[
    \begin{array}{cccc}
        1 &amp; 0 &amp; 0 &amp; 0\\
        0 &amp; 1 &amp; 0 &amp; 0\\
        0 &amp; 0 &amp; 1 &amp; 0\\
        0 &amp; 0 &amp; 0 &amp; 1
    \end{array}
\right] =\end{split}\]</div>
<div class="math">
\[\begin{split}= \left[
    \begin{array}{cccc}
        0 &amp; -1 &amp; 1 &amp; -1\\
        -1 &amp; 0 &amp; -1 &amp; 1\\
        1 &amp; -1 &amp; 0 &amp; -1\\
        -1 &amp; 1 &amp; -1 &amp; 0
    \end{array}
\right]\end{split}\]</div>
<p><span class="math">\(I\)</span> is the identity matrix and <span class="math">\(I \in \Bbb R^{n \times n}\)</span>, where <span class="math">\(n\)</span> is a number of features in the input vector.</p>
<p>When we have one stored vector inside the weights we don’t really need to remove 1s from the diagonal.
The main problem would appear when we have more than one vector stored in the weights.
Each value on the diagonal would be equal to the number of stored vectors in it.</p>
</div>
<div class="section" id="recovery-from-memory">
<h3><a class="toc-backref" href="#id9">Recovery from memory</a></h3>
<p>The main advantage of Autoassociative network is that it is able to recover pattern from the memory using just a partial information about the pattern.
There are already two main approaches to this situation, synchronous and asynchronous.
We are going to master both of them.</p>
<div class="section" id="synchronous">
<h4><a class="toc-backref" href="#id10">Synchronous</a></h4>
<p>Synchronous approach is much more easier for understanding, so we are going to look at it firstly.
To recover your pattern from memory you just need to multiply the weight matrix by the input vector.</p>
<div class="math">
\[\begin{split}\begin{align*}
    s = {W}\cdot{x}=
    \left[
    \begin{array}{cccc}
      w_{11} &amp; w_{12} &amp; \ldots &amp; w_{1n}\\
      w_{21} &amp; w_{22} &amp; \ldots &amp; w_{2n}\\
      \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
      w_{n1} &amp; w_{n2} &amp; \ldots &amp; w_{nn}
    \end{array}
    \right]
    \left[
    \begin{array}{c}
      x_1\\
      x_2\\
      \vdots\\
      x_n
    \end{array}
    \right]
    =
\end{align*}\end{split}\]</div>
<div class="math">
\[\begin{split}\begin{align*}
    =
    \left[
        \begin{array}{c}
          w_{11}x_1+w_{12}x_2 + \cdots + w_{1n} x_n\\
          w_{21}x_1+w_{22}x_2 + \cdots + w_{2n} x_n\\
          \vdots\\
          w_{n1}x_1+w_{n2}x_2 + \cdots + w_{nn} x_n\\
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>Let’s analyze the result.
We summed up all information from the weights where each value can be any integer with an absolute value equal to or smaller than the number of patterns inside the network.
It’s clear that total sum value for <span class="math">\(s_i\)</span> is not necessary equal to -1 or 1, so we have to make additional operations that will make bipolar vector from the vector <span class="math">\(s\)</span>.</p>
<p>Let’s think about this product operation.
What does it actualy do?
Basically after training procedure we saved our pattern dublicated <span class="math">\(n\)</span> times (where <span class="math">\(n\)</span> is a number of input vector features) inside the weight.
When we store more patterns we get interception between them (it’s called a <strong>crosstalk</strong>) and each pattern add some noise to other patterns.
So, after perfoming product matrix between <span class="math">\(W\)</span> and <span class="math">\(x\)</span> for each value from the vector <span class="math">\(x\)</span> we’ll get a recovered vector with a little bit of noise.
For <span class="math">\(x_1\)</span> we get a first column from the matrix <span class="math">\(W\)</span>, for the <span class="math">\(x_2\)</span> a second column, and so on.
Then we sum up all vectors together.
This operation can remind you of voting.
For example we have 3 vectors.
If the first two vectors have 1 in the first position and the third one has -1 at the same position, the winner should be 1.
We can perform the same procedure with <span class="math">\(sign\)</span> function.
So the output value should be 1 if total value is greater then zero and -1 otherwise.</p>
<div class="math">
\[\begin{split}sign(x) = \left\{
    \begin{array}{lr}
        &amp;1 &amp;&amp; : x \ge 0\\
        &amp;-1 &amp;&amp; : x &lt; 0
    \end{array}
\right.\\\end{split}\]\[y = sign(s)\]</div>
<p>That’s it.
Now <span class="math">\(y\)</span> store the recovered pattern from the input vector <span class="math">\(x\)</span>.</p>
<p>Maybe now you can see why we can’t use zeros in the input vectors.
In <cite>voting</cite> procedure we use each row that was multiplied by bipolar number, but if values had been zeros they would have ignored columns from the weight matrix and we would have used only values related to ones in the input pattern.</p>
<p>Of course you can use 0 and 1 values and sometime you will get the correct result, but this approach give you much worse results than explained above.</p>
</div>
<div class="section" id="asynchronous">
<h4><a class="toc-backref" href="#id11">Asynchronous</a></h4>
<p>Previous approach is good, but it has some limitations.
If you change one value in the input vector it can change your output result and value won’t converge to the known pattern.
Another popular approach is an <strong>asynchronous</strong>.
This approach is more likely to remind you of real memory.
At the same time in network activates just one random neuron instead of all of them.
In terms of neural networks we say that <strong>neuron fires</strong>.
We iteratively repeat this operation multiple times and after some point network will converge to some pattern.</p>
<p>Let’s look at this example:
Consider that we already have a weight matrix <span class="math">\(W\)</span> with one pattern <span class="math">\(x\)</span>  inside of it.</p>
<div class="math">
\[\begin{split}\begin{align*}
    W =
    \left[
    \begin{array}{cccc}
      0 &amp; 1 &amp; -1 \\
      1 &amp; 0 &amp; -1 \\
      -1 &amp; -1 &amp; 0
    \end{array}
    \right]
\end{align*}\end{split}\]</div>
<div class="math">
\[\begin{split}\begin{align*}
    x =
    \left[
        \begin{array}{c}
          1\\
          1\\
          -1
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>Let’s assume that we have a vector <span class="math">\(x^{'}\)</span> from which we want to recover the pattern.</p>
<div class="math">
\[\begin{split}\begin{align*}
    x^{'} =
    \left[
        \begin{array}{c}
          1\\
          -1\\
          -1
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>In first iteration one neuron fires.
Let it be the second one.
So we multiply the first column by this selected value.</p>
<div class="math">
\[\begin{split}\begin{align*}
    x^{'}_2 =
    sign(\left[
        \begin{array}{c}
          1 &amp; -1 &amp; -1
        \end{array}
    \right] \cdot \left[
        \begin{array}{c}
          1\\
          0\\
          -1
        \end{array}
    \right]) = sign(2) = 1
\end{align*}\end{split}\]</div>
<p>And after this operation we set up a new value into the input vector <span class="math">\(x\)</span>.</p>
<div class="math">
\[\begin{split}\begin{align*}
    x^{'} =
    \left[
        \begin{array}{c}
          1\\
          1\\
          -1
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>As you can see after first iteration value is exacly the same as <span class="math">\(x\)</span> but we can keep going.
In second iteration random neuron fires again.
Let’s pretend that this time it was the third neuron.</p>
<div class="math">
\[\begin{split}\begin{align*}
    x^{'}_3 =
    sign(\left[
        \begin{array}{c}
          1 &amp; 1 &amp; -1
        \end{array}
    \right] \cdot \left[
        \begin{array}{c}
          -1\\
          -1\\
          0
        \end{array}
    \right]) = sign(-2) = -1
\end{align*}\end{split}\]</div>
<p><span class="math">\(x^{'}_3\)</span> is exacly the same as in the <span class="math">\(x^{'}\)</span> vector so we don’t need to update it.
We can repeat it as many times as we want, but we will be getting the same value.</p>
</div>
</div>
</div>
<div class="section" id="memory-limit">
<h2><a class="toc-backref" href="#id12">Memory limit</a></h2>
<p>Obviously, you can’t store infinite number of vectors inside the network.
There are two good rules of thumb.</p>
<p>Concider that <span class="math">\(n\)</span> is the dimension (number of features) of your input vector and <span class="math">\(m\)</span> is the number of patterns that you want to store in the network.
The first rule gives us a simple ration between <span class="math">\(m\)</span> and <span class="math">\(n\)</span>.</p>
<div class="math">
\[m \approx 0.18 n\]</div>
<p>The main problem with this rule is that proof assumes that stored vectors inside the weight are completly random with an equal probability.
Unfortunately, that is not always true.
Let’s suppose we save some images of numbers from 0 to 9.
Pictures are black and white, so we can encode them in bipolar vectors.
Will the probabilities be the same for seeing as many white pixels as black ones?
Usually no.
More likely that number of white pixels would be greater than number of black ones.
Before use this rule you have to think about type of your input patterns.</p>
<p>The second rule uses a logarithmic proportion.</p>
<div class="math">
\[m = \left \lfloor \frac{n}{2 \cdot log(n)} \right \rfloor\]</div>
<p>Both of these rules are good assumtions about the nature of data and its possible limits in memory.
Of course you can find situations when these rules will fail.</p>
</div>
<div class="section" id="hallucinations">
<h2><a class="toc-backref" href="#id13">Hallucinations</a></h2>
<p>Hallucinations is one of the main problems in the <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.
Sometimes network output can be something that we hasn’t taught it.</p>
<p>To understand this phenomena we should firstly define the Hopfield energy function.</p>
<div class="math">
\[E = -\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij} x_i x_j + \sum_{i=1}^{n} \theta_i x_i\]</div>
<p>Where <span class="math">\(w_{ij}\)</span> is a weight value on the <span class="math">\(i\)</span>-th row and <span class="math">\(j\)</span>-th column.
<span class="math">\(x_i\)</span> is a <span class="math">\(i\)</span>-th values from the input vector <span class="math">\(x\)</span>.
<span class="math">\(\theta\)</span> is a threshold.
Threshold defines the bound to the sign function.
For this reason <span class="math">\(\theta\)</span> is equal to 0 for the <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.
In terms of a linear algebra we can write formula for the <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> energy Function more simpler.</p>
<div class="math">
\[E = -\frac{1}{2} x^T W x\]</div>
<p>But linear algebra notation works only with the <span class="math">\(x\)</span> vector, we can’t use matrix <span class="math">\(X\)</span> with multiple input patterns instead of the <span class="math">\(x\)</span> in this formula.
For the energy function we’re always interested in finding a minimum value, for this reason it has minus sign at the beggining.</p>
<p>Let’s try to visualize it.
Assume that values for vector <span class="math">\(x\)</span> can be continous in order and we can visualize them using two parameters.
Let’s pretend that we have two vectors <cite>[1, -1]</cite> and <cite>[-1, 1]</cite> stored inside the network.
Below you can see the plot that visualizes energy function for this situation.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/energy-function.png"><img alt="Energy function visualization for the network with two neurons" src="_images/energy-function.png" style="width: 80%;"/></a>
</div>
<p>As you can see we have two minimum values at the same points as those patterns that are already stored inside the network.
But between these two patterns function creates a saddle point somewhere at the point with coordinates <span class="math">\((0, 0)\)</span>.
In this case we can’t stick to the points <span class="math">\((0, 0)\)</span>.
But in situation with more dimensions this saddle points can be at the level of available values and they could be hallucination.
Unfortunately, we are very limited in terms of numbers of dimensions we could plot, but the problem is still the same.</p>
<p>Full source code for this plot you can find on <a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/memory/dhn_energy_func.py">github</a></p>
</div>
<div class="section" id="example">
<h2><a class="toc-backref" href="#id14">Example</a></h2>
<p>Now we are ready for a more practical example.
Let’s define a few images that we are going to teach the network.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">draw_bin_image</span><span class="p">(</span><span class="n">image_matrix</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">image_matrix</span><span class="o">.</span><span class="n">tolist</span><span class="p">():</span>
<span class="gp">... </span>        <span class="k">print</span><span class="p">(</span><span class="s1">'| '</span> <span class="o">+</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">' *'</span><span class="p">[</span><span class="n">val</span><span class="p">]</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">row</span><span class="p">))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">zero</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span>
<span class="gp">... </span><span class="p">])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">one</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
<span class="gp">... </span><span class="p">])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">two</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span><span class="p">])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">zero</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * * *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">|   * * *</span>
</pre></div>
</div>
<p>We have 3 images, so now we can train network with these patterns.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">zero</span><span class="p">,</span> <span class="n">one</span><span class="p">,</span> <span class="n">two</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">DiscreteHopfieldNetwork</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">'sync'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>That’s all. Now to make sure that network has memorized patterns right we can define the broken patterns and check how the network will recover them.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">half_zero</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">half_zero</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * * *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">|</span>
<span class="go">|</span>
<span class="go">|</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">half_two</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">half_two</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|</span>
<span class="go">|</span>
<span class="go">|</span>
<span class="go">|   * *</span>
<span class="go">| *</span>
<span class="go">| * * * * *</span>
</pre></div>
</div>
<p>Now we can reconstruct pattern from the memory.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_zero</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * * *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">|   * * *</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_two</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">| * * *</span>
<span class="go">|       *</span>
<span class="go">|       *</span>
<span class="go">|   * *</span>
<span class="go">| *</span>
<span class="go">| * * * * *</span>
</pre></div>
</div>
<p>Cool! Network catches the pattern right.</p>
<p>But not always we will get the correct answer. Let’s define another broken pattern and check network output.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">half_two</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span><span class="p">])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_two</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="go">|   * *</span>
<span class="go">| *   *</span>
<span class="go">| * * * * *</span>
</pre></div>
</div>
<p>We hasn’t clearly taught the network to deal with such pattern. But if we look closer, it looks like mixed pattern of numbers 1 and 2.</p>
<p>This problem we can solve using the asynchronous network approach. We don’t necessary need to create a new network, we can just simply switch its mode.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">environment</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">environment</span><span class="o">.</span><span class="n">reproducible</span><span class="p">()</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="s1">'async'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span><span class="o">.</span><span class="n">n_times</span> <span class="o">=</span> <span class="mi">400</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_two</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_two</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">| * * *</span>
<span class="go">|       *</span>
<span class="go">|       *</span>
<span class="go">|   * *</span>
<span class="go">| *</span>
<span class="go">| * * * *</span>
</pre></div>
</div>
<p>Our broken pattern is really close to the minimum of 1 and 2 patterns. Randomization helps us choose direction but it’s not nessesary the right one, especialy when the broken pattern is close to 1 and 2 at the same time.</p>
<p>Check last output with number two again. Is that a realy valid pattern for number 2? Final symbol in output is wrong. We are not able to recover patter 2 from this network, because input vector is always much closer to the minimum that looks very similar to pattern 2.</p>
<p>In plot below you can see first 200 iterations of the recovery procedure. Energy value was decreasing after each iteration until it reached the local minimum where pattern is equal to 2.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/hopfield-energy-vis.png"><img alt="Asynchronous Discrete Hopfield Network energy update after each iteration" src="_images/hopfield-energy-vis.png" style="width: 80%;"/></a>
</div>
<p>And finally we can look closer to the network memory using Hinton diagram.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">plots</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Hinton diagram"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plots</span><span class="o">.</span><span class="n">hinton</span><span class="p">(</span><span class="n">dhnet</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/hinton-diagram.png"><img alt="Asynchronous Discrete Hopfield Network energy update after each iteration" src="_images/hinton-diagram.png" style="width: 80%;"/></a>
</div>
<p>This graph above shows the network weight matrix and all information stored inside of it. Hinton diagram is a very simple technique for the weight visualization in neural networks. Each value encoded in square where its size is an absolute value from the weight matrix and color shows the sign of this value. White is a positive and black is a negative. Usualy Hinton diagram helps identify some patterns in the weight matrix.</p>
<p>Let’s go back to the graph. What can you say about the network just by looking at this picture? First of all you can see that there is no squares on the diagonal. That is because they are equal to zero. The second important thing you can notice is that the plot is symmetrical. But that is not all that you can withdraw from the graph. Can you see different patterns? You can find rows or columns with exacly the same values, like the second and third columns. Fifth column is also the same but its sign is reversed. Now look closer to the antidiagonal. What can you say about it? If you are thinking that all squares are white - you are right. But why is that true? Is there always the same patterns for each memory matrix? No, it is a special property of patterns that we stored inside of it. If you draw a horizontal line in the middle of each image and look at it you will see that values are opposite symmetric. For instance, <span class="math">\(x_1\)</span> opposite symmetric to <span class="math">\(x_{30}\)</span>, <span class="math">\(x_2\)</span> to <span class="math">\(x_{29}\)</span>, <span class="math">\(x_3\)</span> to <span class="math">\(x_{28}\)</span> and so on. Zero pattern is a perfect example where each value have exacly the same opposite symmetric pair. One is almost perfect except one value on the <span class="math">\(x_2\)</span> position. Two is not clearly opposite symmetric. But if you check each value you will find that more than half of values are symmetrical. Combination of those patterns gives us a diagonal with all positive values. If we have all perfectly opposite symmetric patterns then squares on the antidiagonal will have the same length, but in this case pattern for number 2 gives a little bit of noise and squares have different sizes.</p>
<p>Properties that we’ve reviewed so far are just the most interesting and maybe other patterns you can encounter on your own.</p>
</div>
<div class="section" id="more-reading">
<h2><a class="toc-backref" href="#id15">More reading</a></h2>
<p>In addition you can read another article about a ‘<a class="reference internal" href="2015/09/21/password_recovery.html#password-recovery"><span>Password recovery</span></a>‘ from the memory using the <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.</p>
</div>
<div class="section" id="references">
<h2><a class="toc-backref" href="#id16">References</a></h2>
<table class="docutils footnote" frame="void" id="id3" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[1]</a></td><td>R. Rojas. Neural Networks. In Associative Networks. pp. 311 - 336, 1996.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Math4IQB. (2013, November 17). Hopfield Networks. Retrieved
from <a class="reference external" href="https://www.youtube.com/watch?v=gfPUWwBkXZY">https://www.youtube.com/watch?v=gfPUWwBkXZY</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>R. Callan. The Essence of Neural Networks. In Pattern Association. pp. 84 - 98, 1999.</td></tr>
</tbody>
</table>
</div>

        </div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Yurii Shevchuk</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="tags/memory.html">memory</a>, <a href="tags/unsupervised.html">unsupervised</a></span>
        </div>
        <div class="comments">
            <a href="http://neupy.com/2015/09/20/discrete_hopfield_network.html#disqus_thread" data-disqus-identifier="2015/09/20/discrete_hopfield_network">Leave a comment</a>
        </div></div><div class="separator post_separator"></div><div class="timestamp postmeta">
            <span>July 04, 2015</span>
        </div>
        <div class="section">
            <span id="boston-house-price"/><h1><a href="2015/07/04/boston_house_prices_dataset.html">Predict prices for houses in the area of Boston</a></h1>
<p>For this article we are going to predict house prices using Conjugate Gradient algorithm.</p>
<p>For the beginning we should load the data.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_boston</span><span class="p">()</span>
<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
<p>Let’s look closer into the data.</p>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.00632</td>
      <td>18</td>
      <td>2.31</td>
      <td>0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1</td>
    </tr>
    <tr>
      <td>0.02731</td>
      <td>0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2</td>
    </tr>
    <tr>
      <td>0.02729</td>
      <td>0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2</td>
    </tr>
    <tr>
      <td>0.03237</td>
      <td>0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3</td>
    </tr>
    <tr>
      <td>0.06905</td>
      <td>0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>296</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <td>242</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <td>242</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <td>222</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <td>222</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
  </tbody>
</table><p>Data contains 14 columns.
The last column <span class="docutils literal"><span class="pre">MEDV</span></span> is a median value of owner-occupied homes in $1000’s.
The goal is to predict this prices.
Other columns we can use for Neural Network training.
All columns description you can find below.</p>
<ul class="simple">
<li>CRIM     per capita crime rate by town</li>
<li>ZN       proportion of residential land zoned for lots over 25,000 sq.ft.</li>
<li>INDUS    proportion of non-retail business acres per town</li>
<li>CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</li>
<li>NOX      nitric oxides concentration (parts per 10 million)</li>
<li>RM       average number of rooms per dwelling</li>
<li>AGE      proportion of owner-occupied units built prior to 1940</li>
<li>DIS      weighted distances to five Boston employment centres</li>
<li>RAD      index of accessibility to radial highways</li>
<li>TAX      full-value property-tax rate per $10,000</li>
<li>PTRATIO  pupil-teacher ratio by town</li>
<li>B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town</li>
<li>LSTAT    % lower status of the population</li>
</ul>
<p>From data set description we can find that there are 13 continuous attributes (including “class” attribute “MEDV”) and 1 binary-valued attribute.
There is no multiple categorical data, so we don’t need to change feature dimension.
But we already have one problem.
If you look closer, you will find that every column has its own data range.
This situation is a bad thing for Neural Network training, because input values ​​make different contributions to the calculation of the output values.
Bigger values will be more important for Network which can be perceived as invalid assumption based on data.
For example in the first row, in the table above, column <span class="docutils literal"><span class="pre">B</span></span> contains value <cite>396.90</cite> and column <span class="docutils literal"><span class="pre">CRIM</span></span> - <cite>0.00632</cite>.
To fix this issue we should transfrom all columns to get similar ranges.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>

<span class="n">data_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">target_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">MinMaxScaler</span><span class="p">()</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">data_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">target_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<p>After transformation data looks like this.</p>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>...</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.000000</td>
      <td>0.18</td>
      <td>0.067815</td>
      <td>0</td>
      <td>0.314815</td>
      <td>...</td>
    </tr>
    <tr>
      <td>0.000236</td>
      <td>0.00</td>
      <td>0.242302</td>
      <td>0</td>
      <td>0.172840</td>
      <td>...</td>
    </tr>
    <tr>
      <td>0.000236</td>
      <td>0.00</td>
      <td>0.242302</td>
      <td>0</td>
      <td>0.172840</td>
      <td>...</td>
    </tr>
    <tr>
      <td>0.000293</td>
      <td>0.00</td>
      <td>0.063050</td>
      <td>0</td>
      <td>0.150206</td>
      <td>...</td>
    </tr>
    <tr>
      <td>0.000705</td>
      <td>0.00</td>
      <td>0.063050</td>
      <td>0</td>
      <td>0.150206</td>
      <td>...</td>
    </tr>
  </tbody>
</table><p>All the data is now in the range between 0 and 1.</p>
<p>Then we should split our data set into train and validation.
We use 85% of data for train.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">environment</span>

<span class="n">environment</span><span class="o">.</span><span class="n">reproducible</span><span class="p">()</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.85</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Now we are ready to build Neural Network which will predict house prices.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span><span class="p">,</span> <span class="n">layers</span>

<span class="n">cgnet</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">ConjugateGradient</span><span class="p">(</span>
    <span class="n">connection</span><span class="o">=</span><span class="p">[</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="mi">13</span><span class="p">),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">],</span>
    <span class="n">search_method</span><span class="o">=</span><span class="s1">'golden'</span><span class="p">,</span>
    <span class="n">show_epoch</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">addons</span><span class="o">=</span><span class="p">[</span><span class="n">algorithms</span><span class="o">.</span><span class="n">LinearSearch</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/cgnet-init.png"><img alt="Conjgate Gradient train" src="_images/cgnet-init.png" style="width: 80%;"/></a>
</div>
<p>We define network with one hidden layer.
Input size for this layer is 50.
This value is just a guess.
For better and more accurate result we should choose it with other methods, but for now we can use this value.
As the main algorithm we take Conjugate Gradient.
This implementation of backpropagation is a little bit different from main interpretation of Conjugate Gradient.
For GradientDescent implementation we can’t guarantee that we get the local minimum in n-th steps (where <cite>n</cite> is the dimension).
To optimize it we should use linear search.
It will fix and set up better steps for Conjugate Gradient.</p>
<p>Now we are going to train the network.
For training we set up 100 epochs.
Also we will add test data into training function to check validation error on every epoch.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="n">cgnet</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/cgnet-train.png"><img alt="Conjgate Gradient train" src="_images/cgnet-train.png" style="width: 80%;"/></a>
</div>
<p>To make sure that all training processes go in a right way we can check erros updates while the training is in process.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">plots</span>
<span class="n">plots</span><span class="o">.</span><span class="n">error_plot</span><span class="p">(</span><span class="n">cgnet</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/cgnet-error-plot.png"><img alt="Conjgate Gradient train" src="_images/cgnet-error-plot.png" style="width: 80%;"/></a>
</div>
<p>Error minimization procedure looks fine.
The problem is, that last error doesn’t show us the full picture of prediction accuracy.
Our output is always between zero and one and we count the results always into Mean Square Error.
To fix it, we are going to inverse our transformation for predicted and actual values and for accuracy measurment we will use Root Mean Square Logarithmic Error (RMSLE).</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy.estimators</span> <span class="kn">import</span> <span class="n">rmsle</span>

<span class="n">y_predict</span> <span class="o">=</span> <span class="n">cgnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">error</span> <span class="o">=</span> <span class="n">rmsle</span><span class="p">(</span><span class="n">target_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span>
              <span class="n">target_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_predict</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we can see that our error approximately equals to <cite>0.22</cite> which is pretty small.
In the table below you can find 10 randomly chosen errors.</p>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>Actual</th>
      <th>Predicted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>31.2</td>
      <td>27.5</td>
    </tr>
    <tr>
      <td>18.7</td>
      <td>18.5</td>
    </tr>
    <tr>
      <td>20.1</td>
      <td>18.5</td>
    </tr>
    <tr>
      <td>17.2</td>
      <td>9.5</td>
    </tr>
    <tr>
      <td>8.3</td>
      <td>9.5</td>
    </tr>
    <tr>
      <td>50.0</td>
      <td>41.0</td>
    </tr>
    <tr>
      <td>42.8</td>
      <td>32.0</td>
    </tr>
    <tr>
      <td>20.5</td>
      <td>18.5</td>
    </tr>
    <tr>
      <td>16.8</td>
      <td>23.0</td>
    </tr>
    <tr>
      <td>11.8</td>
      <td>9.5</td>
    </tr>
  </tbody>
</table><p>The results are good for the first network implementation.
There are a lot of things which we can do to improve network results, but we will discuss them in an another article.</p>

        </div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Yurii Shevchuk</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="tags/supervised.html">supervised</a>, <a href="tags/backpropagation.html">backpropagation</a>, <a href="tags/regression.html">regression</a>, <a href="tags/tutorials.html">tutorials</a></span>
        </div>
        <div class="comments">
            <a href="http://neupy.com/2015/07/04/boston_house_prices_dataset.html#disqus_thread" data-disqus-identifier="2015/07/04/boston_house_prices_dataset">Leave a comment</a>
        </div></div><div class="separator post_separator"></div><div class="timestamp postmeta">
            <span>July 04, 2015</span>
        </div>
        <div class="section">
            <h1><a href="2015/07/04/visualize_backpropagation_algorithms.html"><a class="toc-backref" href="#id3">Visualize Algorithms based on the Backpropagation</a></a></h1>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#visualize-algorithms-based-on-the-backpropagation" id="id3">Visualize Algorithms based on the Backpropagation</a><ul>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#checking-data" id="id4">Checking data</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#initialize-contour" id="id5">Initialize contour</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#id1" id="id6">Visualize algorithms based on the Backpropagation</a><ul>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#gradient-descent" id="id7">Gradient Descent</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#momentum" id="id8">Momentum</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#rprop" id="id9">RPROP</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#irprop" id="id10">iRPROP+</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#gradient-descent-and-golden-search" id="id11">Gradient Descent and Golden Search</a></li>
</ul>
</li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#bring-them-all-together" id="id12">Bring them all together</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#summary" id="id13">Summary</a></li>
</ul>
</li>
</ul>
</div>
<p>In this article we will be testing different algorithms based on the backpropagation method, visualizing them and trying to figure out some important features from the plots.</p>
<div class="section" id="checking-data">
<h2><a class="toc-backref" href="#id4">Checking data</a></h2>
<p>First of all we need to define simple dataset which contains 6 points with two features.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
<span class="p">])</span>
<span class="n">target_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
<span class="p">])</span>
</pre></div>
</div>
<p>So we can make a scatter plot and look closer at this dots.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">input_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">input_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">target_data</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/bp-vis-scatter.png"><img alt="Dataset scatter plot" src="_images/bp-vis-scatter.png" style="width: 80%;"/></a>
</div>
<p>From the figure above we can clearly see that all dots are linearly separable and we are able to solve this problem with simple perceptron. But the goal of this article is to make clear visualization of learning process for different algorithm based on the backpropagation method, so the problem has to be as simple as possible, because in other cases it will be complex to visualize.</p>
<p>So, since the problem is linear separable we can solve it without hidden layers in network. There are two features and two classes, so we can build network which will take 2 input values and will produce 1 output. We need just two weights, so we can visualize them in contour plot.</p>
</div>
<div class="section" id="initialize-contour">
<h2><a class="toc-backref" href="#id5">Initialize contour</a></h2>
<p>I won’t  add all code related to the plots building in the article. In case if you are interested you can check the main script <a class="reference external" href="https://github.com/itdxer/neupy/blob/master/examples/mlp/gd_algorithms_visualization.py">here</a>.</p>
<a class="reference internal image-reference" href="_images/raw-contour-plot.png"><img alt="Approximation function contour plot" class="align-center" src="_images/raw-contour-plot.png" style="width: 80%;"/></a>
<p>The plot above shows error rate that depends on the network’s weights. The best result corresponds to the smallest error value. The best weights combination for this problem should be near the bottom right corner in the white area.</p>
<p>Next, we are going to look at 5 algorithms based on the Backpropagation. They are:</p>
<ul class="simple">
<li>Gradient descent</li>
<li>Momentum</li>
<li>RPROP</li>
<li>iRPROP+</li>
<li>Gradient Descent + Golden Search</li>
</ul>
<p>Let’s define start point for our algorithms. I’ve chosen the <cite>(-4, -4)</cite> point, because at this point network gives bad results and it will be interesting to observe the learning progress from a bad initialization point. In the script you can set up any other starting point you like.</p>
<p>This function will train the network until the error will be smaller than <cite>0.125</cite>. Every network starts at place with coordinates <cite>(-4, -4)</cite> and finishes near the point with the error value lower than <cite>0.125</cite>.</p>
</div>
<div class="section" id="id1">
<h2><a class="toc-backref" href="#id6">Visualize algorithms based on the Backpropagation</a></h2>
<div class="section" id="gradient-descent">
<h3><a class="toc-backref" href="#id7">Gradient Descent</a></h3>
<p>Let’s primarily check <a class="reference internal" href="apidocs/neupy.algorithms.gd.base.html#neupy.algorithms.gd.base.GradientDescent" title="neupy.algorithms.gd.base.GradientDescent"><span class="xref py py-class docutils literal"><span class="pre">Gradient</span> <span class="pre">Descent</span></span></a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/bp-steps.png"><img alt="Weight update steps for the Gradient Descent" src="_images/bp-steps.png" style="width: 80%;"/></a>
</div>
<p>Gradient Descent got to the value close to 0.125 using 797 steps and this black curve is just tiny steps of gradient descent algorithm. We can zoom it and look even closer.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/bp-steps-zoom.png"><img alt="Zoomed weight update steps for the Gradient Descent" src="_images/bp-steps-zoom.png" style="width: 80%;"/></a>
</div>
<p>Now we can see some information about gradient descent algorithm. All steps for gradient descent algorithm have approximately similar magnitude. Their direction doesn’t vary because contours in the zoomed picture are parallel to each other and in it we can see that there are still a lot of steps that are needed to be made to achieve the minimum. Also we can see that small vectors are perpendicular to the contour.</p>
<p>The problem is that the step size is a very sensitive parameter for the gradient descent. In typical problem we won’t be able to visualize the learning progress and we won’t have an ability to see that our updates over the epochs are inefficient. For this result I’ve used step size equal to <span class="docutils literal"><span class="pre">0.3</span></span>, but if we increased it to <span class="docutils literal"><span class="pre">10</span></span> we would reach our goal in <span class="docutils literal"><span class="pre">25</span></span> steps. I haven’t added any improvements to make a fair comparison to other algorithms in the summary chapter.</p>
</div>
<div class="section" id="momentum">
<h3><a class="toc-backref" href="#id8">Momentum</a></h3>
<p>Now let’s look at another very popular algorithm - <a class="reference internal" href="apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/momentum-steps.png"><img alt="Momentum steps" src="_images/momentum-steps.png" style="width: 80%;"/></a>
</div>
<p><a class="reference internal" href="apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> got to the value close to 0.125 by 92 steps, which is more than 8 times less than for the gradient descent. The basic idea behind <a class="reference internal" href="apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> algorithm is that it accumulates gradients from the previous epochs. It means that if the gradient has the same direction after each epoch weight update vector magnitude will increase. But if the gradient stars changing its direction weight update vector magnitude will decrease. Check the figure again. Imagine that you’re standing at a skatepark. Than you throw a ball into a half-pipe in a way that makes it roll smoothly on the surface. While it rolls down the gravity force drags it down and it makes the ball roll faster and faster. Let’s get back to the <a class="reference internal" href="apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> algorithm and try to find these properties in the plot.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/momentum-steps-zoom.png"><img alt="Momentum steps zoom on increasing weight update size" src="_images/momentum-steps-zoom.png" style="width: 80%;"/></a>
</div>
<p>When we zoom the plot we can see that the direction for weight update vectors is almost the same and gradient’s direction doesn’t change after every epoch. In the picture above the vector which is the last on the right is bigger than the first one on the same plot on the left. Since it always moves forward it speeds up.</p>
<p>Let’s get back to the ball example. What happens when the ball reaches the pit of the half-pipe for the first time? Will it stop? Of course not. Ball gained enough speed for moving. So it will go up. But after that the ball will start to slow down and its amplitude will become smaller and smaller, because of the gravity force, that will continue to push it down to the pit and eventually it will stop to move. Let’s try to find the similar behavior in the same plot.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/momentum-steps-zoom-decrease.png"><img alt="Momentum steps zoom on decreasing weight update size" src="_images/momentum-steps-zoom-decrease.png" style="width: 80%;"/></a>
</div>
<p>From the figure above it’s clear that weight update magnitude became smaller. Like a ball that slows down and changes its direction towards the minimum.</p>
<p>And finally to make it even more intuitive you can check weight update trajectory in 3D plot. It looks much more like the ball and half-pipe in skatepark analogy.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/momentum-3d-trajectory.png"><img alt="Momentum 3D trajectory" src="_images/momentum-3d-trajectory.png" style="width: 80%;"/></a>
</div>
</div>
<div class="section" id="rprop">
<h3><a class="toc-backref" href="#id9">RPROP</a></h3>
<p><a class="reference internal" href="apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> makes fewer steps to reach the specified minimum point, but we still can do better. Next algorithm that we are going to check is <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/rprop-steps.png"><img alt="RPROP steps" src="_images/rprop-steps.png" style="width: 80%;"/></a>
</div>
<p>This improvement looks impressive. Now we are able to see steps without zooming. We got almost the same value as before using just 20 steps, which is approximately 5 times less than <a class="reference internal" href="apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> and approximately 40 times less than <a class="reference internal" href="apidocs/neupy.algorithms.gd.base.html#neupy.algorithms.gd.base.GradientDescent" title="neupy.algorithms.gd.base.GradientDescent"><span class="xref py py-class docutils literal"><span class="pre">Gradient</span> <span class="pre">Descent</span></span></a>.</p>
<p>Now we are going to figure out what are the main features of <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a>. We can notice just by looking at the plot above <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> has a unique step for each weight. There are just two steps for each weight in the input layer for this network. <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> will increase the step size if gradient don’t change the sign compare to previous epoch, and it will decrease otherwise.</p>
<p>Let’s check a few first weight updates.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/rprop-first-11-steps.png"><img alt="RPROP first 11 steps" src="_images/rprop-first-11-steps.png" style="width: 80%;"/></a>
</div>
<p>From the figure above you can see that first 11 updates have the same direction, so both steps increase their value after each iteration. For the first epoch steps are equal to the same value which we set up at network initialization step. In further iterations they increased by the same constant factor, so after six iteration they got bigger, but they are still equal because they move in one direction all the time.</p>
<p>Now let’s check the next epochs from the figure below. At the 12th epoch gradient changed the direction, but steps are still the same in value. But we can clearly see that gradient changed the sign for the second weight. <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> updated the step after weight had updated, so the step for the second weight should be smaller for the 13th epoch.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/rprop-11th-to-14th-epochs.png"><img alt="RPROP from 11th to 14th steps" src="_images/rprop-11th-to-14th-epochs.png" style="width: 80%;"/></a>
</div>
<p>Now let’s look at the 13th epoch. It shows us how gradient sign difference at the 12th epoch updated steps. Now the steps are not equal. From the picture above we can see that update on the second weight (y axis) is smaller than on the first weight (x axis).</p>
<p>At the 16th epoch gradient on y axis changed the sign again. Network decreased by constant factor and updated for the second weight at the 17th epoch would be smaller than at the 16th.</p>
<p>To train your intuition you can check the other epochs updates and try to figure out how steps depend on the direction.</p>
</div>
<div class="section" id="irprop">
<h3><a class="toc-backref" href="#id10">iRPROP+</a></h3>
<p><a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.IRPROPPlus" title="neupy.algorithms.gd.rprop.IRPROPPlus"><span class="xref py py-class docutils literal"><span class="pre">iRPROP+</span></span></a> is almost the same algorithm as <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> except a small alteration.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/irprop-plus-steps.png"><img alt="iRPROP+ steps" src="_images/irprop-plus-steps.png" style="width: 80%;"/></a>
</div>
<p>As in <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> algorithm <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.IRPROPPlus" title="neupy.algorithms.gd.rprop.IRPROPPlus"><span class="xref py py-class docutils literal"><span class="pre">iRPROP+</span></span></a> make exactly the same first 11 steps.</p>
<p>Now let’s look at the 12th step in the figure below.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/irprop-plus-second-part.png"><img alt="iRPROP+ second part" src="_images/irprop-plus-second-part.png" style="width: 80%;"/></a>
</div>
<p>Second weight (on the y axis) didn’t change the value. At the same epoch <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> changed the gradient comparing to the previous epoch and just decreased step value after weight update whereas, <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.IRPROPPlus" title="neupy.algorithms.gd.rprop.IRPROPPlus"><span class="xref py py-class docutils literal"><span class="pre">iRPROP+</span></span></a> disabled weight update for current epoch (set it up to <cite>0</cite>). And of course it also decreased the step for the second weight. Also you can find that vector for the 12th epoch that looks smaller than for the <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> algorithm, because we ignored the second weight update. If we check the x axis update size we will find that it has the same value as in <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> algorithm.</p>
<p>At 13th epoch network again included second weight into the update process, because compared to the previous epoch gradient didn’t change its sign.</p>
<p>The nice thing about this algorithm is that it tries to move in a new direction instead of going back and force and trying to redo updates from the previous epochs.</p>
</div>
<div class="section" id="gradient-descent-and-golden-search">
<h3><a class="toc-backref" href="#id11">Gradient Descent and Golden Search</a></h3>
<p>The last algorithm that I want to show is a <a class="reference internal" href="apidocs/neupy.algorithms.step_update.linear_search.html#neupy.algorithms.step_update.linear_search.LinearSearch" title="neupy.algorithms.step_update.linear_search.LinearSearch"><span class="xref py py-class docutils literal"><span class="pre">Golden</span> <span class="pre">Search</span></span></a>. This algorithm is not able to train a network by itself, but it can help other algorithms to do it better. I will use Gradient Descent to show the huge improvement that gives <a class="reference internal" href="apidocs/neupy.algorithms.step_update.linear_search.html#neupy.algorithms.step_update.linear_search.LinearSearch" title="neupy.algorithms.step_update.linear_search.LinearSearch"><span class="xref py py-class docutils literal"><span class="pre">Golden</span> <span class="pre">Search</span></span></a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/grad-descent-and-gold-search-steps.png"><img alt="Gradient Descent with Golden Search steps" src="_images/grad-descent-and-gold-search-steps.png" style="width: 80%;"/></a>
</div>
<p>It took just two steps to reach the goal. Let’s check the first step. <a class="reference internal" href="apidocs/neupy.algorithms.step_update.linear_search.html#neupy.algorithms.step_update.linear_search.LinearSearch" title="neupy.algorithms.step_update.linear_search.LinearSearch"><span class="xref py py-class docutils literal"><span class="pre">Golden</span> <span class="pre">Search</span></span></a> helps to find the best step size that can be in a specified direction. So basically, it just tries multiple combinations until it finds the best one. As you can see from the plot the first step size is almost perfect for the specified direction. If you went farther you would increase the error.</p>
<p>The main disadvantage of <a class="reference internal" href="apidocs/neupy.algorithms.step_update.linear_search.html#neupy.algorithms.step_update.linear_search.LinearSearch" title="neupy.algorithms.step_update.linear_search.LinearSearch"><span class="xref py py-class docutils literal"><span class="pre">Golden</span> <span class="pre">Search</span></span></a> is a time complexity. It will take a while to find a good step in specified direction. So for the more complicated networks it can take a lot of time to find a perfect step size.</p>
</div>
</div>
<div class="section" id="bring-them-all-together">
<h2><a class="toc-backref" href="#id12">Bring them all together</a></h2>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/all-algorithms-steps.png"><img alt="All algorithms steps" src="_images/all-algorithms-steps.png" style="width: 80%;"/></a>
</div>
</div>
<div class="section" id="summary">
<h2><a class="toc-backref" href="#id13">Summary</a></h2>
<table border="1" class="docutils" id="id2">
<caption><span class="caption-text">Summary table</span></caption>
<colgroup>
<col width="50%"/>
<col width="50%"/>
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Algorithm</th>
<th class="head">Number of epochs</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Gradient Descent</td>
<td>797</td>
</tr>
<tr class="row-odd"><td>Momentum</td>
<td>92</td>
</tr>
<tr class="row-even"><td>RPROP</td>
<td>20</td>
</tr>
<tr class="row-odd"><td>iRPROP+</td>
<td>17</td>
</tr>
<tr class="row-even"><td>Gradient Descent + Golden Search</td>
<td>2</td>
</tr>
</tbody>
</table>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/compare-number-of-epochs.png"><img alt="Compare number of epochs" src="_images/compare-number-of-epochs.png" style="width: 80%;"/></a>
</div>
<p>There is no perfect algorithm for neural network that can solve all problems. All of them have their own pros and cons. Some of the algorithms can be memory or computationally overwhelming and you have to choose an algorithm depending on the task you want to solve.</p>
<p>All code is available at <a class="reference external" href="https://github.com/itdxer/neupy/blob/master/examples/mlp/gd_algorithms_visualization.py">GitHub</a>. You can play around with the script and set up different learning algorithms and hyperparameters. More algorithms you can find at NeuPy’s <a class="reference internal" href="pages/cheatsheet.html#cheat-sheet"><span>Cheat sheet</span></a>.</p>
</div>

        </div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Yurii Shevchuk</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="tags/supervised.html">supervised</a>, <a href="tags/backpropagation.html">backpropagation</a>, <a href="tags/visualization.html">visualization</a></span>
        </div>
        <div class="comments">
            <a href="http://neupy.com/2015/07/04/visualize_backpropagation_algorithms.html#disqus_thread" data-disqus-identifier="2015/07/04/visualize_backpropagation_algorithms">Leave a comment</a>
        </div></div><div class="archive_link">
        <a href="archive.html"> &mdash; Blog Archive &mdash; </a>
    </div></article><aside class="sidebar"><section><div class="widget">
    <h1>Recent Articles</h1>
    <ul><li>
            <a href="2018/03/26/making_art_with_growing_neural_gas.html">Making Art with Growing Neural Gas</a>
        </li><li>
            <a href="2017/12/17/sofm_text_style.html">Create unique text-style with SOFM</a>
        </li><li>
            <a href="2017/12/13/sofm_art.html">The Art of SOFM</a>
        </li><li>
            <a href="2017/12/09/sofm_applications.html">Self-Organizing Maps and Applications</a>
        </li><li>
            <a href="2016/12/17/hyperparameter_optimization_for_neural_networks.html">Hyperparameter optimization for Neural Networks</a>
        </li></ul>
</div></section><section><div class="widget">
    <h1>Install NeuPy</h1>
    <div class="highligh-bash">
        <div class="highlight">
            <pre>pip install neupy</pre>
        </div>
    </div>
    <p>
        <div>Learn more about NeuPy reading <a href="docs/tutorials.html">tutorials</a> and <a href="pages/documentation.html">documentation</a>.</div>
    </p>
</div></section><section><div class="widget" id="searchbox" role="search">
    <h1><a href="#searchbox">Search</a></h1>
    <form action="search.html" method="get">
          <div class="box">
            <div class="search-input-container">
                <span class="icon"><i class="fa fa-search"></i></span>
                <input type="search" name="q" id="search" placeholder="Search..." />
            </div>
          </div>
    </form>
</div></section><section><div class="widget">
    <h1>Issues and feature requests</h1>
    <p>
        If you find a bug or want to suggest a new feature feel free to
        <a href="https://github.com/itdxer/neupy/issues/new">create an issue</a>
        on Github
    </p>
</div></section><section><div class="widget">
    <h1>Old NeuPy versions</h1>
    <p>
        <div>Documentation for the old NeuPy versions you can find <a href="pages/versions.html">here</a>.</div>
    </p>
</div></section></aside></div> <!-- #main --></div> <!-- #main-container -->

        <div class="footer-container" role="contentinfo"><footer class="wrapper">&copy; Copyright 2015 - 2017, Yurii Shevchuk. Powered by <a href="http://www.tinkerer.me/">Tinkerer</a> and <a href="http://sphinx.pocoo.org/">Sphinx</a>.</footer></div> <!-- footer-container -->

      </div> <!--! end of #container --><script type="text/javascript">    var disqus_shortname = "neupy";    disqus_count();</script><!--[if lt IE 7 ]>
          <script src="//ajax.googleapis.com/ajax/libs/chrome-frame/1.0.3/CFInstall.min.js"></script>
          <script>window.attachEvent('onload',function(){CFInstall.check({mode:'overlay'})})</script>
        <![endif]-->
    </body>
</html>